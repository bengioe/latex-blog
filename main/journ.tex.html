
<!DOCTYPE html>
<meta charset="utf-8">
<html>
  <head>
    <title>Papers, papers, things</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    <script src="https://fpcdn.s3.amazonaws.com/apps/polygon-tools/0.4.6/polygon-tools.min.js" type="text/javascript"></script>
    
    <link rel="stylesheet" href="main.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
  </head>
  <body>
   <div class="content">
   <center><a href="http://folinoid.com/">[Home]</a></center>
     <br/>
<br/>
Welcome to my loosely organized collection of papers, notes and thoughts. I made this public, but use it with discretion, some of the summaries may be from me glancing at the abstract for 17.4 seconds.<br/>

<ul class="toc"><li>1 <a href="#s1">Reinforcement Learning</a></li>
<li>1.1 <a href="#s2">Generalization in Deep RL</a></li>
<li>1.2 <a href="#s3">Generalization RL theory</a></li>
<li>1.3 <a href="#s4">Interference (+in RL)</a></li>
<li>1.4 <a href="#s5">Deadly Triad</a></li>
<li>1.5 <a href="#s6">Regularization in RL</a></li>
<li>1.6 <a href="#s7">Multi-step returns in RL</a></li>
<li>1.7 <a href="#s8">Methods</a></li>
<li>1.8 <a href="#s9">Model-based</a></li>
<li>1.9 <a href="#s10">Hierarchical RL</a></li>
<li>2 <a href="#s11">Thoughts on Research</a></li>
<li>3 <a href="#s12">DNN Theory</a></li>
<li>3.1 <a href="#s13">Generalization in Deep SL</a></li>
<li>3.2 <a href="#s14">Learning dynamics</a></li>
<li>3.3 <a href="#s15">Self-Supervised Learning</a></li>
<li>3.4 <a href="#s16">Meta Learning + Continual Learning</a></li>
<li>3.5 <a href="#s17">The nature of deep nets</a></li>
<li>4 <a href="#s18">DNN architectures</a></li>
<li>5 <a href="#s19">RL Theory</a></li>
<li>6 <a href="#s20">Imitation Learning</a></li>
<li>7 <a href="#s21">Staleness-corrected momentum -- related work</a></li>
<li>8 <a href="#s22">ICML 2020</a></li>
<li>8.1 <a href="#s23">Generalization in Deep RL</a></li>
<li>8.2 <a href="#s24">DNN generalization</a></li>
<li>8.3 <a href="#s25">Model-based RL</a></li>
<li>9 <a href="#s26">NeurIPS 2020</a></li>
<li>9.1 <a href="#s27">Deep RL</a></li>
<li>9.2 <a href="#s28">RL Theory/Linear-land</a></li>
<li>9.3 <a href="#s29">Model-based RL</a></li>
<li>9.4 <a href="#s30">Goal-based/hindsight</a></li>
<li>9.5 <a href="#s31">Deep Learning</a></li>
<li>9.6 <a href="#s32">Learning Theory</a></li>
<li>9.7 <a href="#s33">AI Alignment</a></li>
<li>9.8 <a href="#s34">Datasets</a></li>
<li>9.9 <a href="#s35">Workshops</a></li></ul><br/>
<a name="s1"></a><h3>1 Reinforcement Learning</h3><br/>

<a name="s2"></a><h4>1.1 Generalization in Deep RL</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Generalization+in+reinforcement+learning:+Successful+examples+using+sparse+coarse+coding&btnG="><span>Richard S Sutton<br/><i>Generalization in reinforcement learning: Successful examples using sparse coarse coding</i>, 1996</span>Richard S Sutton (1996)</a>, an early success of TD<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="false">(</mo><mi>λ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">(\lambda)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mopen">(</span><span class="mord mathnormal">λ</span><span class="mclose">)</span></span></span></span> with linear+sparse tile coding.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Protecting+against+evaluation+overfitting+in+empirical+reinforcement+learning&btnG="><span>Shimon Whiteson, Brian Tanner, Matthew E Taylor, Peter Stone<br/><i>Protecting against evaluation overfitting in empirical reinforcement learning</i>, 2011</span>Shimon Whiteson et al. (2011)</a>, seems it's been known for a while that we're doing RL evaluation wrong -- tries to formalize how we should train and test RL agents on distributions of environments.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Revisiting+the+arcade+learning+environment:+Evaluation+protocols+and+open+problems+for+general+agents&btnG="><span>Marlos C Machado, Marc G Bellemare, Erik Talvitie, Joel Veness, Matthew Hausknecht, Michael Bowling<br/><i>Revisiting the arcade learning environment: Evaluation protocols and open problems for general agents</i>, 2018</span>Marlos C Machado et al. (2018)</a>, adding stochasticity (off-policy steps) to agents can break them, but training them with the stochasticity recovers performance (a priori some form of generalization).
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Zero-shot+task+generalization+with+multi-task+deep+reinforcement+learning&btnG="><span>Junhyuk Oh, Satinder Singh, Honglak Lee, Pushmeet Kohli<br/><i>Zero-shot task generalization with multi-task deep reinforcement learning</i>, 2017</span>Junhyuk Oh et al. (2017)</a>, with embedding (of instrutions) regularizations, agents can generalize to new instruction sequences on navigation tasks.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Generalization+and+Regularization+in+DQN&btnG="><span>Jesse Farebrother, Marlos C Machado, Michael Bowling<br/><i>Generalization and Regularization in DQN</i>, 2018</span>Jesse Farebrother et al. (2018)</a>, L2/dropout can help on Atari, evaluation on different levels
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=A+study+on+overfitting+in+deep+reinforcement+learning&btnG="><span>Chiyuan Zhang, Oriol Vinyals, Remi Munos, Samy Bengio<br/><i>A study on overfitting in deep reinforcement learning</i>, 2018</span>Chiyuan Zhang et al. (2018)</a>, DRL agents can generalize on random mazes, but need lots of mazes to not overfit. Regularization and stochasticity may not help with generalization, still unclear. Similarly, <a class="tooltip" href="https://scholar.google.com/scholar?q=Quantifying+generalization+in+reinforcement+learning&btnG="><span>Karl Cobbe, Oleg Klimov, Chris Hesse, Taehoon Kim, John Schulman<br/><i>Quantifying generalization in reinforcement learning</i>, 2018</span>Karl Cobbe et al. (2018)</a>, even on procedurally generated environments, DRL agents can easily overfit on their training set unless regularized (or unless a large number of them are generated). <a class="tooltip" href="https://scholar.google.com/scholar?q=Illuminating+generalization+in+deep+reinforcement+learning+through+procedural+level+generation&btnG="><span>Niels Justesen, Ruben Rodriguez Torrado, Philip Bontrager, Ahmed Khalifa, Julian Togelius, Sebastian Risi<br/><i>Illuminating generalization in deep reinforcement learning through procedural level generation</i>, 2018</span>Niels Justesen et al. (2018)</a> also find that generating environments helps with generalization to similar environments.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Assessing+Generalization+in+Deep+Reinforcement+Learning&btnG="><span>Charles Packer, Katelyn Gao, Jernej Kos, Philipp Krähenbühl, Vladlen Koltun, Dawn Song<br/><i>Assessing Generalization in Deep Reinforcement Learning</i>, 2018</span>Charles Packer et al. (2018)</a>,  DRL agents have some extrapolation capabilities when trained on a distribution of environment parameters (e.g. pole mass in CartPole), but they are limited.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=A+dissection+of+overfitting+and+generalization+in+continuous+reinforcement+learning&btnG="><span>Amy Zhang, Nicolas Ballas, Joelle Pineau<br/><i>A dissection of overfitting and generalization in continuous reinforcement learning</i>, 2018</span>Amy Zhang et al. (2018)</a>, using fixed sets of random seeds for continuous state/action MDPs seems to be a good measure of generalization/overfitting. As expected more training seeds is better. Adding a model-based objective seems to hurt generalization.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Measuring+and+Characterizing+Generalization+in+Deep+Reinforcement+Learning&btnG="><span>Sam Witty, Jun Ki Lee, Emma Tosch, Akanksha Atrey, Michael Littman, David Jensen<br/><i>Measuring and Characterizing Generalization in Deep Reinforcement Learning</i>, 2018</span>Sam Witty et al. (2018)</a>, forcing the agent to start from unreachable or off-policy states breaks them, training from there is hard for the DQN algorithm.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Investigating+Generalisation+in+Continuous+Deep+Reinforcement+Learning&btnG="><span>Chenyang Zhao, Olivier Siguad, Freek Stulp, Timothy M Hospedales<br/><i>Investigating Generalisation in Continuous Deep Reinforcement Learning</i>, 2019</span>Chenyang Zhao et al. (2019)</a>, "standard algorithms and architectures generalize poorly in the face of noise and environmental shift".
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Generalization+in+reinforcement+learning+with+selective+noise+injection+and+information+bottleneck&btnG="><span>Maximilian Igl, Kamil Ciosek, Yingzhen Li, Sebastian Tschiatschek, Cheng Zhang, Sam Devlin, Katja Hofmann<br/><i>Generalization in reinforcement learning with selective noise injection and information bottleneck</i>, 2019</span>Maximilian Igl et al. (2019)</a>, noise regularization (e.g. dropout) but with noise on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> turned off for TD bootstrapping step and policy gradient computation, and noise on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> turned off for rollouts (less exploratory?). Also proposes IB-based AC method.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=The+Impact+of+Non-stationarity+on+Generalisation+in+Deep+Reinforcement+Learning&btnG="><span>Maximilian Igl, Gregory Farquhar, Jelena Luketina, Wendelin Boehmer, Shimon Whiteson<br/><i>The Impact of Non-stationarity on Generalisation in Deep Reinforcement Learning</i>, 2020</span>Maximilian Igl et al. (2020)</a>, non-stationarity experiments on supervised and RL, propose to retrain actor-critic from scratch by distillation once in a while (ITER). Lesson: DNNs get "stuck" in a parameter region and even though they can adapt to non-stationarity (we know because the train reward mostly stays the same to the baseline) they do so by sacrificing "generalization power" (increased performance on the test envs)
    </li><li><a class="tooltip" href="https://arxiv.org/abs/1902.10250"><span>Justin Fu, Aviral Kumar, Matthew Soh, Sergey Levine<br/><i>Diagnosing bottlenecks in deep q-learning algorithms</i>, 2019</span>Justin Fu et al. (2019)</a>, ``function approximation error is not a major problem in Q-learning algorithms, but only when the representationalcapacity of the function approximator is high'', low-divergence rate, oracle-based early stopping helps combat overfitting, non-stationarity doesn't seem to affect performance (weird), high-entropy rather than on-policy data sampling is better.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=In+Support+of+Over-Parametrization+in+Deep+Reinforcement+Learning:+an+Empirical+Study&btnG="><span>Brady Neal, Ioannis Mitliagkas<br/><i>In Support of Over-Parametrization in Deep Reinforcement Learning: an Empirical Study</i>, 2019</span>Brady Neal et al. (2019)</a>, on non-generalizing environments (mountain car, cartpole, acrobot), one can abritrarily increase the width without any signs of overfitting. Is this surprising though, given that the training set and test set are the same?
    </li><li><a class="tooltip" href="https://openreview.net/forum?id=HJli2hNKDH"><span>Xingyou Song, Yiding Jiang, Stephen Tu, Yilun Du, Behnam Neyshabur<br/><i>Observational Overfitting in Reinforcement Learning</i>, 2020</span>Xingyou Song et al. (2020)</a>, agents can overfit to suprious observations (e.g. timer in games) but increasing capacity always provides implicit regularization (Overparametrization improves generalization for CoinRun).
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Image+augmentation+is+all+you+need:+Regularizing+deep+reinforcement+learning+from+pixels&btnG="><span>Ilya Kostrikov, Denis Yarats, Rob Fergus<br/><i>Image augmentation is all you need: Regularizing deep reinforcement learning from pixels</i>, 2020</span>Ilya Kostrikov et al. (2020)</a>, it seems possible to just augment the pixel inputs by noise and random shifts and get state-of-the-art on Atari 100k with DQN (also averaging both target <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Q</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">Q&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.946332em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">Q</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span></span></span></span> over multiple image transformations helps).
    </li><li><a class="tooltip" href="https://distill.pub/2020/understanding-rl-vision"><span>Jacob Hilton, Nick Cammarata, Shan Carter, Gabriel Goh, Chris Olah<br/><i>Understanding RL Vision</i>, 2020</span>Jacob Hilton et al. (2020)</a>, this paper has more, but one result, training on enough CoinRun levels eventually leads to interpretability (and perhaps not coincidentally to better performance on the test set levels). Perhaps all along in deep RL there was a diversity issue that made progress clunky and uninformative.
</li></ul><br/>
<a name="s3"></a><h4>1.2 Generalization RL theory</h4>
<ul><li>(<a class="tooltip" href="https://scholar.google.com/scholar?q=Is+a+Good+Representation+Sufficient+for+Sample+Efficient+Reinforcement+Learning?&btnG="><span>Simon S Du, Sham M Kakade, Ruosong Wang, Lin F Yang<br/><i>Is a Good Representation Sufficient for Sample Efficient Reinforcement Learning?</i>, 2019</span>Simon S Du et al., 2019</a>), there exists classes of MDPs (tree-like) where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mn>2</mn><mi>H</mi></msup></mrow><annotation encoding="application/x-tex">2^H</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8413309999999999em;vertical-align:0em;"></span><span class="mord"><span class="mord">2</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.08125em;">H</span></span></span></span></span></span></span></span></span></span></span> samples are needed, generalization seems improbable, even with "good features" (I still disagree with the conclusion of this paper but it seems to be beyond my mathematical comprehension abilities, so who knows.).
    </li><li>(<a class="tooltip" href="https://scholar.google.com/scholar?q=On+the+Generalization+Gap+in+Reparameterizable+Reinforcement+Learning&btnG="><span>Huan Wang, Stephan Zheng, Caiming Xiong, Richard Socher<br/><i>On the Generalization Gap in Reparameterizable Reinforcement Learning</i>, 2019</span>Huan Wang et al., 2019</a>), part of the paper differentiates intrinsic from external generalization error. For some (reparameterizable) MDPs, "the objective can always be reformulated so that the policy only affects the reward instead of the underlying distribution."
    </li><li>(<a class="tooltip" href="https://scholar.google.com/scholar?q=A+Geometric+Perspective+on+Optimal+Representations+for+Reinforcement+Learning&btnG="><span>Marc G. Bellemare, Will Dabney, Robert Dadashi, Adrien Ali Taiga, Pablo Samuel Castro, Nicolas Le Roux, Dale Schuurmans, Tor Lattimore, Clare Lyle<br/><i>A Geometric Perspective on Optimal Representations for Reinforcement Learning</i>, 2019</span>Marc G. Bellemare et al., 2019</a>), for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">n</span></span></span></span> states, all possible value functions form a polytope. This polytope has extrema (kind of like the vertices) and instead of learning a representation for all possible value functions, one can learn a representation for those extrema only (adversarial value functions).\\
    fun observation:
    <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><munder><mo><mi>max</mi><mo>⁡</mo></mo><mrow><mi>π</mi><mo>∈</mo><mi mathvariant="script">P</mi></mrow></munder><munder><mo>∑</mo><mrow><mi>x</mi><mo>∈</mo><mi mathvariant="script">X</mi></mrow></munder><mi>δ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><msup><mi>V</mi><mi>π</mi></msup><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}\max_{\pi\in\mathcal{P}} \sum_{x \in \mathcal{X}}\delta(x) V^\pi(x)\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.671711em;vertical-align:-1.0858555em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.5858555em;"><span style="top:-3.5858555em;"><span class="pstrut" style="height:3.050005em;"></span><span class="mord"><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.43055999999999994em;"><span style="top:-2.355669em;margin-left:0em;"><span class="pstrut" style="height:3em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span><span class="mrel mtight">∈</span><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:0.08222em;">P</span></span></span></span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span><span class="mop">max</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.771701em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.050005em;"><span style="top:-1.8556639999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">x</span><span class="mrel mtight">∈</span><span class="mord mtight"><span class="mord mathcal mtight" style="margin-right:0.14643em;">X</span></span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.321706em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.7143919999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03588em;">π</span></span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.0858555em;"><span></span></span></span></span></span></span></span></span></span></span></span>
    yields the optimal policy regardless of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi></mrow><annotation encoding="application/x-tex">\delta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span></span></span></span>, as long as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>δ</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>≥</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">\delta(x)\geq 0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03785em;">δ</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>(<a class="tooltip" href="https://scholar.google.com/scholar?q=Dynamic+Programming+and+Optimal+Control,+Vol.+II:+Approximate+Dynamic+Programming&btnG="><span>Dimitri P Bertsekas<br/><i>Dynamic Programming and Optimal Control, Vol. II: Approximate Dynamic Programming</i>, 2012</span>Dimitri P Bertsekas, 2012</a>)</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+the+Expressivity+of+Neural+Networks+for+Deep+Reinforcement+Learning&btnG="><span>Kefan Dong, Yuping Luo, Tengyu Ma<br/><i>On the Expressivity of Neural Networks for Deep Reinforcement Learning</i>, 2019</span>Kefan Dong et al. (2019)</a><sup><a href="https://icml.cc/virtual/2020/poster/6381">[poster]</a></sup>, one can build MDPs with extremely simple (and easy to learn) models that end up having very nasty Q functions. Since this is presumably the case for domains like Mujoco, they show simple planning can help at test time.
</li></ul><br/>
<a name="s4"></a><h4>1.3 Interference (+in RL)</h4>
(this quantity: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∇</mi><msubsup><mi>J</mi><mi>A</mi><mi>T</mi></msubsup><mi mathvariant="normal">∇</mi><msub><mi>J</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">\nabla J_A^T\nabla J_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1166619999999998em;vertical-align:-0.275331em;"></span><span class="mord">∇</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.424669em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.275331em;"><span></span></span></span></span></span></span><span class="mord">∇</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>)
<ul><li>(<a class="tooltip" href="https://scholar.google.com/scholar?q=Stiffness:+A+New+Perspective+on+Generalization+in+Neural+Networks&btnG="><span>Stanislav Fort, Paweł Krzysztof Nowak, Stanislaw Jastrzebski, Srini Narayanan<br/><i>Stiffness: A New Perspective on Generalization in Neural Networks</i>, 2019</span>Stanislav Fort et al., 2019</a>), interference is linked with generalization, they call it "stiffness" (positive interference), tied with learning rate (Adam), higher learning rate|training more leads to less stiffness (I really think overfitting can be characterized by how orthogonal gradients are. This is evidence.). Other observations: <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>x</mi></mrow><annotation encoding="application/x-tex">x</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">x</span></span></span></span>s from same class are more stiff together, initially at least. Cross-class interference negatively linked to validation score (class confusion from model?).
   
    <ul><li>Hessians and interference seem very related, sub-problem: (<a class="tooltip" href="https://scholar.google.com/scholar?q=Emergent+properties+of+the+local+geometry+of+neural+loss+landscapes&btnG="><span>Stanislav Fort, Surya Ganguli<br/><i>Emergent properties of the local geometry of neural loss landscapes</i>, 2019</span>Stanislav Fort et al., 2019</a>), analyses bulk+outliers structure of Hessian (+different logits are quite orthogonal in many senses)/"loss surface". Empirically observed properties of Hessians can be explained with one model by making reasonable simplifying assumptions. (to revisit)
        </li><li>see also (<a class="tooltip" href="https://scholar.google.com/scholar?q=Measurements+of+Three-Level+Hierarchical+Structure+in+the+Outliers+in+the+Spectrum+of+Deepnet+Hessians&btnG="><span>Vardan Papyan<br/><i>Measurements of Three-Level Hierarchical Structure in the Outliers in the Spectrum of Deepnet Hessians</i>, 2019</span>Vardan Papyan, 2019</a>).
    </li></ul></li><li>(<a class="tooltip" href="https://scholar.google.com/scholar?q=Learning+to+learn+without+forgetting+by+maximizing+transfer+and+minimizing+interference&btnG="><span>Matthew Riemer, Ignacio Cases, Robert Ajemian, Miao Liu, Irina Rish, Yuhai Tu, Gerald Tesauro<br/><i>Learning to learn without forgetting by maximizing transfer and minimizing interference</i>, 2018</span>Matthew Riemer et al., 2018</a>), uses a Reptile-style first-order approximation of the interference to maximize it (positive interference). Multi-task perspective, but works for RL too.
   </li><li>(<a class="tooltip" href="https://optrl2019.github.io/accepted_papers.html"><span>Vincent Liu, Hengshuai Yao, Martha White<br/><i>Toward Understanding Catastrophic Interference inValue-based Reinforcement Learning</i>, 2019</span>Vincent Liu et al., 2019</a>), arrives to same interference quantity with a different approach (Taylor expansion of what I called generalization gain!), some claims wrt to Bellman projection
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Towards+Characterizing+Divergence+in+Deep+Q-Learning&btnG="><span>Joshua Achiam, Ethan Knight, Pieter Abbeel<br/><i>Towards Characterizing Divergence in Deep Q-Learning</i>, 2019</span>Joshua Achiam et al. (2019)</a>, DQN, decomposes deadly triad into maths using NTK/interference trick. Proposes method to find minibatch-optimal inverse kernel, à-la Natural Gradient QL.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Gradient+episodic+memory+for+continual+learning&btnG="><span>David Lopez-Paz, Marc'Aurelio Ranzato<br/><i>Gradient episodic memory for continual learning</i>, 2017</span>David Lopez-Paz et al. (2017)</a>, continual learning, store representative samples of past tasks, disallow destructive interference, measured as <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">∇</mi><msubsup><mi>J</mi><mi>A</mi><mi>T</mi></msubsup><mi mathvariant="normal">∇</mi><msub><mi>J</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">\nabla J_A^T\nabla J_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1166619999999998em;vertical-align:-0.275331em;"></span><span class="mord">∇</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.424669em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.275331em;"><span></span></span></span></span></span></span><span class="mord">∇</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, if an update has destructive interference, project the gradient onto the non-destructive space (solved via quadratic programming).
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+first-order+meta-learning+algorithms&btnG="><span>Alex Nichol, Joshua Achiam, John Schulman<br/><i>On first-order meta-learning algorithms</i>, 2018</span>Alex Nichol et al. (2018)</a>, (Reptile), authors remark that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><msubsup><mi>J</mi><mi>A</mi><mi>T</mi></msubsup><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><msubsup><mi>J</mi><mi>B</mi><mi>T</mi></msubsup><mo stretchy="false">)</mo><mo>=</mo><msub><mi>H</mi><mi>B</mi></msub><mi mathvariant="normal">∇</mi><msub><mi>J</mi><mi>A</mi></msub><mo>+</mo><msub><mi>H</mi><mi>A</mi></msub><mi mathvariant="normal">∇</mi><msub><mi>J</mi><mi>B</mi></msub></mrow><annotation encoding="application/x-tex">\nabla_\theta(\nabla_\theta J_A^T\nabla_\theta J_B^T) = H_B\nabla J_A+H_A\nabla J_B</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1166619999999998em;vertical-align:-0.275331em;"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.424669em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.275331em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.8413309999999999em;"><span style="top:-2.424669em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.13889em;">T</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.275331em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∇</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.08125em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">A</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord">∇</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.32833099999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.09618em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05017em;">B</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span></li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Neural+tangent+kernel:+Convergence+and+generalization+in+neural+networks&btnG="><span>Arthur Jacot, Franck Gabriel, Clément Hongler<br/><i>Neural tangent kernel: Convergence and generalization in neural networks</i>, 2018</span>Arthur Jacot et al. (2018)</a>, This quantity <i>is</i> the neural tangent kernel!
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Ray+interference:+a+source+of+plateaus+in+deep+reinforcement+learning&btnG="><span>Tom Schaul, Diana Borsa, Joseph Modayil, Razvan Pascanu<br/><i>Ray interference: a source of plateaus in deep reinforcement learning</i>, 2019</span>Tom Schaul et al. (2019)</a>, looks at interference between two bandits.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=The+Impact+of+Neural+Network+Overparameterization+on+Gradient+Confusion+and+Stochastic+Gradient+Descent&btnG="><span>Karthik A. Sankararaman, Soham De, Zheng Xu, W. Ronny Huang, Tom Goldstein<br/><i>The Impact of Neural Network Overparameterization on Gradient Confusion and Stochastic Gradient Descent</i>, 2019</span>Karthik A. Sankararaman et al. (2019)</a>, SGD is fast when there is constructive interference; more depth = more negative interference; more width = more positive interference.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+Catastrophic+Interference+in+Atari+2600+Games&btnG="><span>William Fedus, Dibya Ghosh, John D Martin, Marc G Bellemare, Yoshua Bengio, Hugo Larochelle<br/><i>On Catastrophic Interference in Atari 2600 Games</i>, 2020</span>William Fedus et al. (2020)</a>, in harder games like montezuma's revenge, there's a plateau at which more training interferences with predictions at the beginning. Separating states in different contexts (proxied by the current total game score) and training on a single context, one sees an increase in loss for other contexts in Montezuma, but a decrease for most contexts in an easier game like Pong.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Towards+a+practical+measure+of+interference+for+reinforcement+learning&btnG="><span>Vincent Liu, Adam White, Hengshuai Yao, Martha White<br/><i>Towards a practical measure of interference for reinforcement learning</i>, 2020</span>Vincent Liu et al. (2020)</a> "target network frequency is a dominating factor for interference, .x. updates on the last layer result in higher interference than internal updates"
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Phasic+Policy+Gradient&btnG="><span>Karl Cobbe, Jacob Hilton, Oleg Klimov, John Schulman<br/><i>Phasic Policy Gradient</i>, 2020</span>Karl Cobbe et al. (2020)</a>, separating parameterization for <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi></mrow><annotation encoding="application/x-tex">\pi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span></span></span></span> reduces (presumably?) interference and improves performance/generalization. More stuff is going on in that paper, to read.
    </li><li><a class="tooltip" href="https://arxiv.org/abs/2008.13363"><span>Harsh Mehta, Ashok Cutkosky, Behnam Neyshabur<br/><i>Extreme Memorization via Scale of Initialization</i>, 2020</span>Harsh Mehta et al. (2020)</a>, by scaling the initial distribution of DNNs, one can still fit interpolating DNNs, but the generalization error grows. That is, by messing with initialization and activation functions, we can find DNNs that, with an otherwise identical architecture would generalize, fully memorizes its training set. This is reflected in the gradient alignment (class-wise inner product average).
</li></ul><br/>
<a name="s5"></a><h4>1.4 Deadly Triad</h4>
<ul><li>(<a class="tooltip" href="https://scholar.google.com/scholar?q=Deep+Reinforcement+Learning+and+the+Deadly+Triad&btnG="><span>Hado van Hasselt, Yotam Doron, Florian Strub, Matteo Hessel, Nicolas Sonnerat, Joseph Modayil<br/><i>Deep Reinforcement Learning and the Deadly Triad</i>, 2018</span>Hado van Hasselt et al., 2018</a>), recap of recent findings?
    <ul><li>(Deep divergence) Unbounded divergence is uncommon when combining Q-learning and conventional deep reinforcement learning function spaces
        </li><li>(Target networks) There is less divergence when bootstrapping on separate networks
        </li><li>(Overestimation) There is less divergence when correcting for overestimation bias
        </li><li>(Multi-step) Longer multi-step returns will diverge less easily
        </li><li>(Capacity) Larger, more flexible networks will diverge less easily
        </li><li>(prioritisation) Stronger prioritisation of updates will diverge more easily.
    </li></ul></li></ul><br/>
<a name="s6"></a><h4>1.5 Regularization in RL</h4>
without explicitly tackling generalization:
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Massively+parallel+methods+for+deep+reinforcement+learning&btnG="><span>Arun Nair, Praveen Srinivasan, Sam Blackwell, Cagdas Alcicek, Rory Fearon, Alessandro De Maria, Vedavyas Panneershelvam, Mustafa Suleyman, Charles Beattie, Stig Petersen, others<br/><i>Massively parallel methods for deep reinforcement learning</i>, 2015</span>Arun Nair et al. (2015)</a>, Human starts on Atari improve DQN's performance.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Reinforcement+learning+with+unsupervised+auxiliary+tasks&btnG="><span>Max Jaderberg, Volodymyr Mnih, Wojciech Marian Czarnecki, Tom Schaul, Joel Z Leibo, David Silver, Koray Kavukcuoglu<br/><i>Reinforcement learning with unsupervised auxiliary tasks</i>, 2016</span>Max Jaderberg et al. (2016)</a>, training a model with auxiliary tasks improves sample effciency.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Averaged-dqn:+Variance+reduction+and+stabilization+for+deep+reinforcement+learning&btnG="><span>Oron Anschel, Nir Baram, Nahum Shimkin<br/><i>Averaged-dqn: Variance reduction and stabilization for deep reinforcement learning</i>, 2017</span>Oron Anschel et al. (2017)</a>, DQN's value function diverges, instead, create DQN target by averaging Q preds of last <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>K</mi></mrow><annotation encoding="application/x-tex">K</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.07153em;">K</span></span></span></span><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>θ</mi></mrow><annotation encoding="application/x-tex">\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span></span></span></span>s smooths and prevents divergence in Atari.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Temporal+Regularization+for+Markov+Decision+Process&btnG="><span>Pierre Thodoroff, Audrey Durand, Joelle Pineau, Doina Precup<br/><i>Temporal Regularization for Markov Decision Process</i>, 2018</span>Pierre Thodoroff et al. (2018)</a>, by smoothing the value function (as a function of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>v</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">v(s_{t-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">v</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>) in targets, they get better PPO-Atari performance.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=A+Simple+Randomization+Technique+for+Generalization+in+Deep+Reinforcement+Learning&btnG="><span>Kimin Lee, Kibok Lee, Jinwoo Shin, Honglak Lee<br/><i>A Simple Randomization Technique for Generalization in Deep Reinforcement Learning</i>, 2019</span>Kimin Lee et al. (2019)</a>, by training on random conv2D transforms of the data, can train agents to be robust to unseen visual patterns (think sim2real but in texture space). Also kind of works on SL.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=The+utility+of+sparse+representations+for+control+in+reinforcement+learning&btnG="><span>Vincent Liu, Raksha Kumaraswamy, Lei Le, Martha White<br/><i>The utility of sparse representations for control in reinforcement learning</i>, 2019</span>Vincent Liu et al. (2019)</a>, argue/show that sparse representations in the simple RL domains + deepRL learns much faster (consistent with 0-interference). They have distributional regularizations encourage local coactivations (0-interference for states that aren't close).
  </li><li><a class="tooltip" href="https://arxiv.org/abs/2010.14498"><span>Aviral Kumar, Rishabh Agarwal, Dibya Ghosh, Sergey Levine<br/><i>Implicit Under-Parameterization Inhibits Data-Efficient Deep Reinforcement Learning</i>, 2020</span>Aviral Kumar et al. (2020)</a>, DNNs that bootstrap decrease their expressivity, learn low-rank features, doesn't seem fixed by resetting parameters periodically.
    
</li></ul><br/>
<a name="s7"></a><h4>1.6 Multi-step returns in RL</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Bias-Variance+Error+Bounds+for+Temporal+Difference+Updates.&btnG="><span>Michael J Kearns, Satinder P Singh<br/><i>Bias-Variance Error Bounds for Temporal Difference Updates.</i>, 2000</span>Michael J Kearns et al. (2000)</a>, <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi></mrow><annotation encoding="application/x-tex">k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>-step returns and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span> returns, proofs of convergence and explanations as to why their optimal values might be intermediary values. I.e., the best <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span> is problem dependent.
</li></ul><br/>
<a name="s8"></a><h4>1.7 Methods</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Two-Timescale+Networks+for+Nonlinear+Value+Function+Approximation&btnG="><span>Wesley Chung, Somjit Nath, Ajin Joseph, Martha White<br/><i>Two-Timescale Networks for Nonlinear Value Function Approximation</i>, 2019</span>Wesley Chung et al. (2019)</a>, two timescale, train a DNN slowly (with MSTDE? or MSPBE?) then do linear stuff with the top layer.
  </li><li><a class="tooltip" href="https://openreview.net/forum?id=HJlNpoA5YQ"><span>Yifan Wu, George Tucker, Ofir Nachum<br/><i>The Laplacian in {RL}: Learning Representations with Efficient Approximations</i>, 2019</span>Yifan Wu et al. (2019)</a>, <a class="tooltip" href="https://scholar.google.com/scholar?q=A+Laplacian+Framework+for+Option+Discovery+in+Reinforcement+Learning&btnG="><span>Marlos C. Machado, Marc G. Bellemare, Michael Bowling<br/><i>A Laplacian Framework for Option Discovery in Reinforcement Learning</i>, 2017</span>Marlos C. Machado et al. (2017)</a>, laplacian, I don't really know what's going on to go from graph to continuous deep learning but, Laplacian => graph distance instead of L2/other distances, makes learning more "temporal". 
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=When+to+use+parametric+models+in+reinforcement+learning?&btnG="><span>Hado van Hasselt, Matteo Hessel, John Aslanides<br/><i>When to use parametric models in reinforcement learning?</i>, 2019</span>Hado van Hasselt et al. (2019)</a>, when to use parametric models? Hard, it seems replay buffer can be used more efficiently than any model.  Forward planning for behaviour, rather than credit assignment, seems more useful. Interestingly, backward planning is less harmful than forward planning (the hypothesis is that updating imaginary/wrong states with real targets is less bad the updating real states with wrong targets)
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Optimizing+agent+behavior+over+long+time+scales+by+transporting+value&btnG="><span>Chia-Chun Hung, Timothy Lillicrap, Josh Abramson, Yan Wu, Mehdi Mirza, Federico Carnevale, Arun Ahuja, Greg Wayne<br/><i>Optimizing agent behavior over long time scales by transporting value</i>, 2019</span>Chia-Chun Hung et al. (2019)</a>, Temporal Value Transport, basically an intrinsic built back sending value back into time to skip distractor tasks. It's formulation is oddly reminiscent of meta-PG where the reward of the exploration phase is replaced by that of the exploitation phase (<a class="tooltip" href="https://scholar.google.com/scholar?q=Meta-Reinforcement+Learning+of+Structured+Exploration+Strategies&btnG="><span>Abhishek Gupta, Russell Mendonca, YuXuan Liu, Pieter Abbeel, Sergey Levine<br/><i>Meta-Reinforcement Learning of Structured Exploration Strategies</i>, 2018</span>Abhishek Gupta et al., 2018</a>).
  </li><li><a class="tooltip" href="https://openreview.net/pdf?id=HZkfQzqxlbz"><span>Yash Chandak, Georgios Theocharous, Shiv Shankar, Sridhar Mahadevan, Martha White, Philip S Thomas<br/><i>Optimizing for the Future in Non-Stationary MDPs</i>, 2020</span>Yash Chandak et al. (2020)</a>, ``<i>minimizing</i> performance over some of the data from past episodes can be beneficial when searching for a policy that <i>maximizes</i> future performance''. Forecasting future performance and doing PG wrt to predictions of that future performance.
</li></ul><br/>
<a name="s9"></a><h4>1.8 Model-based</h4><br/>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Learning+to+Predict+Without+Looking+Ahead:+World+Models+Without+Forward+Prediction&btnG="><span>C. Daniel Freeman, Luke Metz, David Ha<br/><i>Learning to Predict Without Looking Ahead: World Models Without Forward Prediction</i>, 2019</span>C. Daniel Freeman et al. (2019)</a>, do policy gradient but with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>m</mi><mi>o</mi><mi>d</mi><mi>e</mi><mi>l</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>−</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">model(s_{t-1})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">m</span><span class="mord mathnormal">o</span><span class="mord mathnormal">d</span><span class="mord mathnormal">e</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> as an input with probability <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi></mrow><annotation encoding="application/x-tex">p</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">p</span></span></span></span>, train both model and policy using the same PG loss. The model is thus trained without any forward-looking.
    </li></ul><br/>
<a name="s10"></a><h4>1.9 Hierarchical RL</h4>
<ul><li><a class="tooltip" href="https://proceedings.neurips.cc/paper/2020/file/4a5cfa9281924139db466a8a19291aff-Paper.pdf"><span>Zheng Wen, Doina Precup, Morteza Ibrahimi, Andre Barreto, Benjamin Van Roy, Satinder Singh<br/><i>On Efficiency in Hierarchical Reinforcement Learning</i>, 2020</span>Zheng Wen et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_4a5cfa9281924139db466a8a19291aff.html">[poster]</a></sup>, in MDPs with a certain structure, namely equivalence classes of subgraphs/subMDPs, one can provably have more efficient hierarchical RL, options-based planning.
</li></ul>
    <br/>

<a name="s11"></a><h3>2 Thoughts on Research</h3><br/>
On the role of scientific thought (<a class="tooltip" href="https://www.cs.utexas.edu/users/EWD/transcriptions/EWD04xx/EWD447.html"><span>Edsger S Dijkstra<br/><i>On the role of scientific thought</i>, 1974</span>Edsger S Dijkstra, 1974</a>). Have clearly defined, albeit interlinked, ideas. One should be able to study something in depth and, perhaps most importantly. in isolation. That is not to say that because we study something in isolation it is not linked to other things, it almost always will be, but this isolation requirement is necessary. Was Dijkstra a proto-intersectional thinker? Being able to entertain parallel thoughts is valuable. Looking for these ideas should lead to ``the discovery of useful concepts''.<br/>
It's important for ideas to have a ``general acceptance'', it's important for ideas that we in isolation think are useful to be useful to others as well. For that they need to hear and understand them. But, it's important to be able to ignore this once in a while, otherwise we may be stuck not trying anything.<br/>
The bus ticket theory of genius (<a class="tooltip" href="http://paulgraham.com/genius.html"><span>Paul Graham<br/><i>The Bus Ticket Theory of Genius</i>, 2019</span>Paul Graham, 2019</a>) basically says that geniuses emerge out of being obsessed with something for its own sake rather than out of a desire of impact. That being said, Graham argues we might be able to align what we are obsessed about with impactful things with some rules of thumbs.<br/>
<a name="s12"></a><h3>3 DNN Theory</h3><br/>


<a name="s13"></a><h4>3.1 Generalization in Deep SL</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Train+faster,+generalize+better:+Stability+of+stochastic+gradient+descent&btnG="><span>Moritz Hardt, Ben Recht, Yoram Singer<br/><i>Train faster, generalize better: Stability of stochastic gradient descent</i>, 2016</span>Moritz Hardt et al. (2016)</a>, SGD is stability inducing even in non-convex lipshitz DNNs.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Understanding+deep+learning+requires+rethinking+generalization&btnG="><span>Chiyuan Zhang, Samy Bengio, Moritz Hardt, Benjamin Recht, Oriol Vinyals<br/><i>Understanding deep learning requires rethinking generalization</i>, 2016</span>Chiyuan Zhang et al. (2016)</a>, architectures that generalize can also be trained to memorize
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+large-batch+training+for+deep+learning:+Generalization+gap+and+sharp+minima&btnG="><span>Nitish Shirish Keskar, Dheevatsa Mudigere, Jorge Nocedal, Mikhail Smelyanskiy, Ping Tak Peter Tang<br/><i>On large-batch training for deep learning: Generalization gap and sharp minima</i>, 2016</span>Nitish Shirish Keskar et al. (2016)</a>, large batches generalize less well
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=A+closer+look+at+memorization+in+deep+networks&btnG="><span>Devansh Arpit, Stanisław Jastrzębski, Nicolas Ballas, David Krueger, Emmanuel Bengio, Maxinder S Kanwal, Tegan Maharaj, Asja Fischer, Aaron Courville, Yoshua Bengio, others<br/><i>A closer look at memorization in deep networks</i>, 2017</span>Devansh Arpit et al. (2017)</a>, DNNs seem to learn simpler patterns first, relationship to adv. examples.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Svcca:+Singular+vector+canonical+correlation+analysis+for+deep+learning+dynamics+and+interpretability&btnG="><span>Maithra Raghu, Justin Gilmer, Jason Yosinski, Jascha Sohl-Dickstein<br/><i>Svcca: Singular vector canonical correlation analysis for deep learning dynamics and interpretability</i>, 2017</span>Maithra Raghu et al. (2017)</a>, DNNs learn from bottom up, in fact you can freeze layers, bottom up, during training.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+the+importance+of+single+directions+for+generalization&btnG="><span>Ari Morcos, David GT Barrett, Neil C Rabinowitz, Matthew Botvinick<br/><i>On the importance of single directions for generalization</i>, 2018</span>Ari Morcos et al. (2018)</a>, models that generalize well are more robust to feature ablation
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Insights+on+representational+similarity+in+neural+networks+with+canonical+correlation&btnG="><span>Ari Morcos, Maithra Raghu, Samy Bengio<br/><i>Insights on representational similarity in neural networks with canonical correlation</i>, 2018</span>Ari Morcos et al. (2018)</a>, "networks which generalize converge to more similar representations than networks which memorize, wider networks converge to more similar solutions than narrow networks, trained networks with identical topology but different learning rates converge to distinct clusters with diverse representations."
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=The+Anisotropic+Noise+in+Stochastic+Gradient+Descent:+Its+Behavior+of+Escaping+from+Sharp+Minima+and+Regularization+Effects&btnG="><span>Zhanxing Zhu, Jingfeng Wu, Bing Yu, Lei Wu, Jinwen Ma<br/><i>The Anisotropic Noise in Stochastic Gradient Descent: Its Behavior of Escaping from Sharp Minima and Regularization Effects</i>, 2018</span>Zhanxing Zhu et al. (2018)</a>, SGD creates an anisotropic noise, and our theories cannot treat it as isotropic or they will miss something. There also seems to be some relation between that noise and the curvature of the loss surface where the flatness of SGD is linked to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>T</mi><mi>r</mi><mo stretchy="false">(</mo><mi>H</mi><mi mathvariant="normal">Σ</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">Tr(H \Sigma)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.13889em;">T</span><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.08125em;">H</span><span class="mord">Σ</span><span class="mclose">)</span></span></span></span> with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi mathvariant="normal">Σ</mi></mrow><annotation encoding="application/x-tex">\Sigma</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord">Σ</span></span></span></span> the covariance of the gradient.
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Reconciling+modern+machine+learning+practice+and+the+bias-variance+trade-off&btnG="><span>Mikhail Belkin, Daniel Hsu, Siyuan Ma, Soumik Mandal<br/><i>Reconciling modern machine learning practice and the bias-variance trade-off</i>, 2018</span>Mikhail Belkin et al. (2018)</a>, original double descent paper
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Are+All+Layers+Created+Equal?&btnG="><span>Chiyuan Zhang, Samy Bengio, Yoram Singer<br/><i>Are All Layers Created Equal?</i>, 2019</span>Chiyuan Zhang et al. (2019)</a>, supports that early layers learn earlier, resetting them often causes high loss in performance, less true of latter layers (cls).
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Neural+Complexity+Measures&btnG="><span>Yoonho Lee, Juho Lee, Sung Ju Hwang, Eunho Yang, Seungjin Choi<br/><i>Neural Complexity Measures</i>, 2020</span>Yoonho Lee et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_6e17a5fd135fcaf4b49f2860c2474c7c.html">[poster]</a></sup>, proposes to (meta-)learn a complexity measure with neural networks. Since this measure is differentiable, it can be used as a regularization term in regular training. The description of a model <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>h</mi></mrow><annotation encoding="application/x-tex">h</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">h</span></span></span></span> is (roughly) its output on a set of training and test inputs (plus said inputs).
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Dataset+cartography:+Mapping+and+diagnosing+datasets+with+training+dynamics&btnG="><span>Swabha Swayamdipta, Roy Schwartz, Nicholas Lourie, Yizhong Wang, Hannaneh Hajishirzi, Noah A Smith, Yejin Choi<br/><i>Dataset cartography: Mapping and diagnosing datasets with training dynamics</i>, 2020</span>Swabha Swayamdipta et al. (2020)</a>, (NLP) it seems datasets can be consistently decomposed into easy, ambiguous, and hard examples. This is examplified by the variance of each example's true-class predictions across training. Training (more?) on ambiguous examples has an impact on out-of-distribution (OoD) performance.
</li></ul><br/>
<a name="s14"></a><h4>3.2 Learning dynamics</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Empirical+Analysis+of+the+Hessian+of+Over-Parametrized+Neural+Networks&btnG="><span>Levent Sagun, Utku Evci, V. Ugur Guney, Yann Dauphin, Leon Bottou<br/><i>Empirical Analysis of the Hessian of Over-Parametrized Neural Networks</i>, 2017</span>Levent Sagun et al. (2017)</a>, spectrum of final Hessian, bulk near 0 + a few positive outliers (almost exactly the number of classes), remains unchanged by capacity, large batch produces larger eigenvalues, the trailing negative eigenvalues are extremely small. SGD and GD probably fall into roughly the same basin.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Don't+Decay+the+Learning+Rate,+Increase+the+Batch+Size&btnG="><span>Samuel L. Smith, Pieter-Jan Kindermans, Chris Ying, Quoc V. Le<br/><i>Don't Decay the Learning Rate, Increase the Batch Size</i>, 2017</span>Samuel L. Smith et al. (2017)</a>, decreasing the learning rate and increasing batch size are equivalent (ImageNet ResNet-50), but increased batch size is faster because of parallelism.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Essentially+no+barriers+in+neural+network+energy+landscape&btnG="><span>Felix Draxler, Kambis Veschgini, Manfred Salmhofer, Fred A Hamprecht<br/><i>Essentially no barriers in neural network energy landscape</i>, 2018</span>Felix Draxler et al. (2018)</a>&amp;<a class="tooltip" href="https://scholar.google.com/scholar?q=Loss+Surfaces,+Mode+Connectivity,+and+Fast+Ensembling+of+DNNs&btnG="><span>Timur Garipov, Pavel Izmailov, Dmitrii Podoprikhin, Dmitry P. Vetrov, Andrew Gordon Wilson<br/><i>Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs</i>, 2018</span>Timur Garipov et al. (2018)</a>, DNN solutions are almost easily connected by a low-loss low-error path in parameter space.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+the+Spectral+Bias+of+Neural+Networks&btnG="><span>Nasim Rahaman, Aristide Baratin, Devansh Arpit, Felix Draxler, Min Lin, Fred A. Hamprecht, Yoshua Bengio, Aaron Courville<br/><i>On the Spectral Bias of Neural Networks</i>, 2018</span>Nasim Rahaman et al. (2018)</a>, DNNs learn low-frequency patterns first.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=How+neural+networks+find+generalizable+solutions:+Self-tuned+annealing+in+deep+learning&btnG="><span>Yu Feng, Yuhai Tu<br/><i>How neural networks find generalizable solutions: Self-tuned annealing in deep learning</i>, 2020</span>Yu Feng et al. (2020)</a>, "there is an inverse relation between weight variance and flatness", which is "created" by SGD. Analyses this with mixture of minibatch loss surfaces. I'm confused what "variance" is being talked about though, I feel like they consider the dynamics variance (total variance of a parameter during <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi></mrow><annotation encoding="application/x-tex">N</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">N</span></span></span></span> epochs).
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Gradient+descent+happens+in+a+tiny+subspace&btnG="><span>Guy Gur-Ari, Daniel A Roberts, Ethan Dyer<br/><i>Gradient descent happens in a tiny subspace</i>, 2018</span>Guy Gur-Ari et al. (2018)</a>, "gradients dynamically converge to a very small subspace after a short period of training, .. spanned by a few top eigenvectors of the Hessian (equal to the number of classes in the dataset)"
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=The+lottery+ticket+hypothesis:+Finding+sparse,+trainable+neural+networks&btnG="><span>Jonathan Frankle, Michael Carbin<br/><i>The lottery ticket hypothesis: Finding sparse, trainable neural networks</i>, 2018</span>Jonathan Frankle et al. (2018)</a>, there exists subnetworks (binary masks) that when retrained from the same initial parameters perform as well or better than the original network (compatible with <a class="tooltip" href="https://scholar.google.com/scholar?q=Gradient+descent+happens+in+a+tiny+subspace&btnG="><span>Guy Gur-Ari, Daniel A Roberts, Ethan Dyer<br/><i>Gradient descent happens in a tiny subspace</i>, 2018</span>Guy Gur-Ari et al. (2018)</a>). If the network is too complicated, instead of retraining from 0, retrain from some earlier epoch (<a class="tooltip" href="https://scholar.google.com/scholar?q=The+Lottery+Ticket+Hypothesis+at+Scale&btnG="><span>Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, Michael Carbin<br/><i>The Lottery Ticket Hypothesis at Scale</i>, 2019</span>Jonathan Frankle et al., 2019</a>). Further work shows that DNNs on vision start by being "unstable" (i.e. two copies trained on different data do not initially have low-error linear parameter interpolations, "Linear Mode Connectivity") and then become stable. This explains why a few epochs are needed before a lottery ticket can be identified in more complicated tasks (<a class="tooltip" href="https://scholar.google.com/scholar?q=Linear+Mode+Connectivity+and+the+Lottery+Ticket+Hypothesis&btnG="><span>Jonathan Frankle, Gintare Karolina Dziugaite, Daniel M Roy, Michael Carbin<br/><i>Linear Mode Connectivity and the Lottery Ticket Hypothesis</i>, 2019</span>Jonathan Frankle et al., 2019</a>).
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=An+Investigation+of+Why+Overparameterization+Exacerbates+Spurious+Correlations&btnG="><span>Shiori Sagawa, Aditi Raghunathan, Pang Wei Koh, Percy Liang<br/><i>An Investigation of Why Overparameterization Exacerbates Spurious Correlations</i>, 2020</span>Shiori Sagawa et al. (2020)</a>, look at when overparameterization hurts by capturing spurious correlations, it can be somewhat corrected by oversampling the right parts of the data.
  </li><li><b>double descent</b></li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Deep+double+descent:+Where+bigger+models+and+more+data+hurt&btnG="><span>Preetum Nakkiran, Gal Kaplun, Yamini Bansal, Tristan Yang, Boaz Barak, Ilya Sutskever<br/><i>Deep double descent: Where bigger models and more data hurt</i>, 2019</span>Preetum Nakkiran et al. (2019)</a>, double descent happens in model size but also in epochs
  </li></ul><br/>
<a name="s15"></a><h4>3.3 Self-Supervised Learning</h4>
<ul><li><a class="tooltip" href="https://arxiv.org/abs/2010.00578"><span>Yuandong Tian, Lantao Yu, Xinlei Chen, Surya Ganguli<br/><i>Understanding self-supervised learning with dual deep networks</i>, 2020</span>Yuandong Tian et al. (2020)</a>, looking at the gradient updates of SSL, they show that gradients are stable for features that survive some covariance operator, and that this is balanced for positive and negative pairs somehow (not sure I understand)
</li></ul><br/>

<a name="s16"></a><h4>3.4 Meta Learning + Continual Learning</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Meta-Learning+and+Universality:+Deep+Representations+and+Gradient+Descent+can+Approximate+any+Learning+Algorithm&btnG="><span>Chelsea Finn, Sergey Levine<br/><i>Meta-Learning and Universality: Deep Representations and Gradient Descent can Approximate any Learning Algorithm</i>, 2017</span>Chelsea Finn et al. (2017)</a>, claims that MAML (or parameter-tuning algorithms) generalizes better than RNN-style meta-learners (since they can adapt to any new learning task).
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Task+Agnostic+Continual+Learning+via+Meta+Learning&btnG="><span>Xu He, Jakub Sygnowski, Alexandre Galashov, Andrei A. Rusu, Yee Whye Teh, Razvan Pascanu<br/><i>Task Agnostic Continual Learning via Meta Learning</i>, 2019</span>Xu He et al. (2019)</a>, use the recent inputs to determine context, but via a meta-learning procedure.
  </li><li><a class="tooltip" href="https://openreview.net/forum?id=BklEFpEYwS"><span>Mingzhang Yin, George Tucker, Mingyuan Zhou, Sergey Levine, Chelsea Finn<br/><i>Meta-Learning without Memorization</i>, 2020</span>Mingzhang Yin et al. (2020)</a>, maximize mutual information between prediction and the task information as to not learn one set of parameters that solve all the tasks (force adaptaiton). This can be done as MI on either the weights or the activations, weights seem to work a bit better.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Learning+Causal+Models+Online&btnG="><span>Khurram Javed, Martha White, Yoshua Bengio<br/><i>Learning Causal Models Online</i>, 2020</span>Khurram Javed et al. (2020)</a>, have binary features, compute running average and variance of the feature <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>→</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">\to y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.36687em;vertical-align:0em;"></span><span class="mrel">→</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span> weights to know which features are spurious correlations. To learn the features they propose a random search method but we can probably do better.
  </li><li><a class="tooltip" href="https://www.cell.com/action/showPdf?pii=S1364-6613\%2820\%2930219-9"><span>Raia Hadsell, Dushyant Rao, Andrei A Rusu, Razvan Pascanu<br/><i>Embracing Change: Continual Learning in Deep Neural Networks</i>, 2020</span>Raia Hadsell et al. (2020)</a>, a review of continual learning
  </li><li><a class="tooltip" href="https://arxiv.org/abs/2007.13904"><span>Gunshi Gupta, Karmesh Yadav, Liam Paull<br/><i>La-MAML: Look-ahead Meta Learning for Continual Learning</i>, 2020</span>Gunshi Gupta et al. (2020)</a>, meta-learn per-parameter learning rates, increase the learning rate for constructive interference, decrease it for destructive interference.
</li></ul><br/>
<a name="s17"></a><h4>3.5 The nature of deep nets</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Complexity+of+Linear+Regions+in+Deep+Networks&btnG="><span>Boris Hanin, David Rolnick<br/><i>Complexity of Linear Regions in Deep Networks</i>, 2019</span>Boris Hanin et al. (2019)</a>&amp;<a class="tooltip" href="https://scholar.google.com/scholar?q=Deep+ReLU+Networks+Have+Surprisingly+Few+Activation+Patterns&btnG="><span>Boris Hanin, David Rolnick<br/><i>Deep ReLU Networks Have Surprisingly Few Activation Patterns</i>, 2019</span>Boris Hanin et al. (2019)</a>, the expected number of linear regions in deep nets is <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msubsup><mi>N</mi><mi>h</mi><mi>d</mi></msubsup><mo stretchy="false">)</mo><mo separator="true">,</mo><mtext>  </mtext><msub><mi>N</mi><mi>h</mi></msub><mo>=</mo><msub><mi>n</mi><mrow><mi>h</mi><mo separator="true">,</mo><mn>1</mn></mrow></msub><mo>+</mo><msub><mi>n</mi><mrow><mi>h</mi><mo separator="true">,</mo><mn>2</mn></mrow></msub><mo>+</mo><mi mathvariant="normal">.</mi><mi mathvariant="normal">.</mi></mrow><annotation encoding="application/x-tex">O(N_h^d),\;N_h=n_{h,1}+n_{h,2}+..</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.132216em;vertical-align:-0.2831079999999999em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.849108em;"><span style="top:-2.4168920000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">d</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2831079999999999em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.10903em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.8694379999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mpunct mtight">,</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.8694379999999999em;vertical-align:-0.286108em;"></span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3361079999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">h</span><span class="mpunct mtight">,</span><span class="mord mtight">2</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:0.10556em;vertical-align:0em;"></span><span class="mord">.</span><span class="mord">.</span></span></span></span>,  suprising given the upper bound is at least <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msubsup><mi>n</mi><mi>h</mi><msub><mi>N</mi><mi>l</mi></msub></msubsup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(n_h^{N_l})</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.2302989999999998em;vertical-align:-0.3013079999999999em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">n</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9289909999999999em;"><span style="top:-2.3986920000000005em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">h</span></span></span><span style="top:-3.1506600000000002em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.10903em;">N</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.3448em;"><span style="top:-2.3487714285714287em;margin-left:-0.10903em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight" style="margin-right:0.01968em;">l</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15122857142857138em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.3013079999999999em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>. Seems to remain roughly constant during training
</li></ul><br/>
<a name="s18"></a><h3>4 DNN architectures</h3><br/>
  
<ul><li><a class="tooltip" href="https://arxiv.org/abs/2010.08012"><span>Alex Lamb, Anirudh Goyal, Agnieszka Słowik, Michael Mozer, Philippe Beaudoin, Yoshua Bengio<br/><i>Neural Function Modules with Sparse Arguments: A Dynamic Approach to Integrating Information across Layers</i>, 2020</span>Alex Lamb et al. (2020)</a>, iterative attention architecture, soft-top-k attends over past iterations of layers, claims to make modules specialized.
</li></ul><br/>
<a name="s19"></a><h3>5 RL Theory</h3><br/>
<a class="tooltip" href="https://scholar.google.com/scholar?q=Learning+to+predict+by+the+methods+of+temporal+differences&btnG="><span>Richard S Sutton<br/><i>Learning to predict by the methods of temporal differences</i>, 1988</span>Richard S Sutton (1988)</a> (re)introduces TD learning to the world, interesting observation for me, is that online TD(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>) can be understood as a discounted sum of gradients, but of current parameters.<br/>
<ul><li><a class="tooltip" href="http://www.mit.edu/~dimitrib/Tempdif.pdf"><span>Dimitri P Bertsekas, Sergey Ioffe<br/><i>Temporal differences-based policy iteration and applications in neuro-dynamic programming</i>, 1996</span>Dimitri P Bertsekas et al. (1996)</a>, TD(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>) can be interpreted as doing approximate policy evaluation by doing <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> Bellman projections (bootstrap) where <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi></mrow><annotation encoding="application/x-tex">M</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.10903em;">M</span></span></span></span> is geometrically distributed with parameter <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Infinite-horizon+policy-gradient+estimation&btnG="><span>Jonathan Baxter, Peter L Bartlett<br/><i>Infinite-horizon policy-gradient estimation</i>, 2001</span>Jonathan Baxter et al. (2001)</a>: "The technique  is called the score function or likelihood ratio method and appears to have been first proposed in the sixties (Aleksandrov, Sysoyev, & Shemeneva, 1968; Rubinstein, 1969) for computing performance gradients in i.i.d. (independently and identically distributed) processes."
  </li><li><a class="tooltip" href="https://arxiv.org/abs/1507.01569"><span>A Rupam Mahmood, Huizhen Yu, Martha White, Richard S Sutton<br/><i>Emphatic temporal-difference learning</i>, 2015</span>A Rupam Mahmood et al. (2015)</a>, emphatic TD, which is about methods of updating states with some preference (interest) in mind that weighs parameter updates. For example, one can reweight off-policy data into on-policy data. It seems you can compute these ephatic weights recursively, a bit like value functions.  
  </li><li><a class="tooltip" href="https://arxiv.org/abs/1506.02582"><span>Huizhen Yu<br/><i>On convergence of emphatic temporal-difference learning</i>, 2015</span>Huizhen Yu (2015)</a>, emphatic TD(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>) convergence.
</li></ul><br/>

<a name="s20"></a><h3>6 Imitation Learning</h3><br/>
<ul><li><a class="tooltip" href="https://arxiv.org/pdf/2006.13258.pdf"><span>Paul Barde, Julien Roy, Wonseok Jeon, Joelle Pineau, Christopher Pal, Derek Nowrouzezahrai<br/><i>Adversarial Soft Advantage Fitting: Imitation Learning without Policy Optimization</i>, 2020</span>Paul Barde et al. (2020)</a>, learn to imitate expert trajectories in Adversarial IL setting, but with an alternate generator formulation which doesn't require an RL inner loop.
</li></ul><br/>
<a name="s21"></a><h3>7 Staleness-corrected momentum -- related work</h3>
<b>On momentum, traces, and gradient acceleration in TD</b> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>  </mtext><mtext>  </mtext></mrow><annotation encoding="application/x-tex">\;\;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span></span></span> <br/>
From an RL perspective, our work has some similarity to the so-called eligibility traces mechanism. In particular, in the True Online TD(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>) method of <a class="tooltip" href="https://scholar.google.com/scholar?q=True+Online+TD($\lambda$)&btnG="><span>Harm van Seijen, Rich Sutton<br/><i>True Online TD($\lambda$)</i>, 2014</span>Harm van Seijen et al. (2014)</a>, the authors derive a <i>strict-online</i> update (i.e. weights are updated at every MDP step, using only information from past steps, rather than future information as in the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>-return perspective) where the main mechanism of the derivation lies in finding an update by assuming (at least analytically) that one can ``start over'' and reuse all past data iteratively at each step of training, and then from this analytical assumption derive a recursive update (that doesn't require iterating through all past data). The extra values that have to be kept to compute the recursive updates are then called traces. This is akin to how we conceptualize <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>μ</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\mu^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8831359999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span>, and derive <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>μ</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat \mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.69444em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">μ</span></span><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.22222em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span></span>.<br/>

The conceptual similarities of the work of <a class="tooltip" href="https://scholar.google.com/scholar?q=A+deeper+look+at+planning+as+learning+from+replay&btnG="><span>Harm van Seijen, Rich Sutton<br/><i>A deeper look at planning as learning from replay</i>, 2015</span>Harm van Seijen et al. (2015)</a> with our work are also interesting. There, the authors analyse what ``retraining from scratch'' means (i.e., again, iteratively restarting from <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mn>0</mn></msub><mo>∈</mo><msup><mi mathvariant="double-struck">R</mi><mi>m</mi></msup></mrow><annotation encoding="application/x-tex">\theta_0 \in \mathbb{R}^m</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.30110799999999993em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">0</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∈</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.68889em;vertical-align:0em;"></span><span class="mord"><span class="mord"><span class="mord mathbb">R</span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">m</span></span></span></span></span></span></span></span></span></span></span>) but with some ideal target <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>θ</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\theta^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> (e.g. the current parameters) by redoing sequentially all the TD(0) updates using <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>θ</mi><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">\theta^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span> for all the <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi></mrow><annotation encoding="application/x-tex">n</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">n</span></span></span></span> transitions in a replay buffer, costing <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(nm)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord mathnormal">m</span><span class="mclose">)</span></span></span></span>. They derive an online update showing that one can continually learn at a cost of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>m</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(m^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">m</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span> rather than paying <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>n</mi><mi>m</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(nm)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal">n</span><span class="mord mathnormal">m</span><span class="mclose">)</span></span></span></span> at each step. The proposed update is also reminiscent of our method in that it aims to perform an approximate batch update without computing the entire batch gradient, and also maintains extra momentum-like vectors and matrices. We note that the derivation there only works in the linear TD case. <br/>
In a way, such an insight can be found in the original presentation of TD(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>) of <a class="tooltip" href="https://scholar.google.com/scholar?q=Learning+to+predict+by+the+methods+of+temporal+differences&btnG="><span>Richard S Sutton<br/><i>Learning to predict by the methods of temporal differences</i>, 1988</span>Richard S Sutton (1988)</a>, where the TD(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>) parameter update is written as (equation (4) in the original paper, but with adapted notation):
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi mathvariant="normal">Δ</mi><msub><mi>θ</mi><mi>t</mi></msub><mo>=</mo><mi>α</mi><mo stretchy="false">[</mo><msub><mi>r</mi><mi>t</mi></msub><mo>+</mo><mi>γ</mi><msub><mi>V</mi><msub><mi>θ</mi><mi>t</mi></msub></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mrow><mi>t</mi><mo>+</mo><mn>1</mn></mrow></msub><mo stretchy="false">)</mo><mo>−</mo><msub><mi>V</mi><msub><mi>θ</mi><mi>t</mi></msub></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mi>t</mi></msub><mo stretchy="false">)</mo><mo stretchy="false">]</mo><munderover><mo>∑</mo><mrow><mi>k</mi><mo>=</mo><mn>1</mn></mrow><mi>t</mi></munderover><msup><mi>λ</mi><mrow><mi>t</mi><mo>−</mo><mi>k</mi></mrow></msup><msub><mi mathvariant="normal">∇</mi><msub><mi>θ</mi><mi>t</mi></msub></msub><msub><mi>V</mi><msub><mi>θ</mi><mi>t</mi></msub></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}\Delta\theta_t = \alpha [r_t + \gamma V_{\theta_t}(s_{t+1}) - V_{\theta_t}(s_t)] \sum_{k=1}^t \lambda^{t-k} \nabla_{\theta_t} V_{\theta_t}(s_k)\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.382674em;vertical-align:-1.441337em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.941337em;"><span style="top:-3.941337em;"><span class="pstrut" style="height:3.780561em;"></span><span class="mord"><span class="mord">Δ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mopen">[</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">r</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">+</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.05556em;">γ</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.301108em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">+</span><span class="mord mtight">1</span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.208331em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mclose">]</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.780561em;"><span style="top:-1.8478869999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span><span class="mrel mtight">=</span><span class="mord mtight">1</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.300005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.302113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.441337em;"><span></span></span></span></span></span></span></span></span></span></span></span>
Remark the use of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\theta_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> in the sum; in the linear case since <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">∇</mi><msub><mi>θ</mi><mi>t</mi></msub></msub><msub><mi>V</mi><msub><mi>θ</mi><mi>t</mi></msub></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mo>=</mo><mi>ϕ</mi><mo stretchy="false">(</mo><msub><mi>s</mi><mi>k</mi></msub><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\nabla_{\theta_t} V_{\theta_t}(s_k) = \phi(s_k)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0001em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">ϕ</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, the sum does not depend on <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>θ</mi><mi>t</mi></msub></mrow><annotation encoding="application/x-tex">\theta_t</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.84444em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:-0.02778em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span> and thus can be computed recursively. A posteriori, if one can find a way to cheaply compute <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi mathvariant="normal">∇</mi><msub><mi>θ</mi><mi>t</mi></msub></msub><msub><mi>V</mi><msub><mi>θ</mi><mi>t</mi></msub></msub><mo stretchy="false">(</mo><msub><mi>s</mi><mi>k</mi></msub><mo stretchy="false">)</mo><mtext>  </mtext><mi mathvariant="normal">∀</mi><mi>k</mi></mrow><annotation encoding="application/x-tex">\nabla_{\theta_t} V_{\theta_t}(s_k)\;\forall k</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.0001em;vertical-align:-0.2501em;"></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.29634285714285713em;"><span style="top:-2.357em;margin-left:-0.02778em;margin-right:0.07142857142857144em;"><span class="pstrut" style="height:2.5em;"></span><span class="sizing reset-size3 size1 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.143em;"><span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.2501em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">∀</span><span class="mord mathnormal" style="margin-right:0.03148em;">k</span></span></span></span>, perhaps using the method we propose, it may be an interesting way to perform TD(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>) using a non-linear function approximator.
    
Our analysis is also conceptually related to the work of <a class="tooltip" href="https://scholar.google.com/scholar?q=On+the+worst-case+analysis+of+temporal-difference+learning+algorithms&btnG="><span>Robert E Schapire, Manfred K Warmuth<br/><i>On the worst-case analysis of temporal-difference learning algorithms</i>, 1996</span>Robert E Schapire et al. (1996)</a>, where a worst-case analysis of TD<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mrow></mrow><mo>∗</mo></msup></mrow><annotation encoding="application/x-tex">^*</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.688696em;vertical-align:0em;"></span><span class="mord"><span></span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.688696em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">∗</span></span></span></span></span></span></span></span></span></span></span>(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>) is performed using a <i>best-case learner</i> as the performance upper bound. This is similar to our <i>momentum oracle</i>; just as the momentum oracle is the "optimal" approximation of the accumulation gradients coming from all past training examples, the best-case learner of <a class="tooltip" href="https://scholar.google.com/scholar?q=On+the+worst-case+analysis+of+temporal-difference+learning+algorithms&btnG="><span>Robert E Schapire, Manfred K Warmuth<br/><i>On the worst-case analysis of temporal-difference learning algorithms</i>, 1996</span>Robert E Schapire et al. (1996)</a> is the set parameters that is optimal when one is allowed to look at all past training examples (in contrast to an online TD learner).<br/>
Before moving on from TD(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>), let us remark that eligibility traces and momentum, while similar, estimate different quantities. The usual (non-replacing) traces estimate the exponential moving average of the gradient of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">V_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>, while momentum does so for the objective <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>J</mi></mrow><annotation encoding="application/x-tex">J</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span></span></span></span> (itself a function of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>V</mi><mi>θ</mi></msub></mrow><annotation encoding="application/x-tex">V_\theta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.83333em;vertical-align:-0.15em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span></span></span></span>):
<span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi mathvariant="bold">e</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>λ</mi><mo stretchy="false">)</mo><munderover><mo>∑</mo><mi>k</mi><mi>t</mi></munderover><msup><mi>λ</mi><mrow><mi>t</mi><mo>−</mo><mi>k</mi></mrow></msup><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><msub><mi>V</mi><mi>θ</mi></msub><mo separator="true">,</mo><mtext>  </mtext><mtext>  </mtext><mtext>  </mtext><mtext>  </mtext><msub><mi>μ</mi><mi>t</mi></msub><mo>=</mo><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo><munderover><mo>∑</mo><mi>k</mi><mi>t</mi></munderover><msup><mi>β</mi><mrow><mi>t</mi><mo>−</mo><mi>k</mi></mrow></msup><msub><mi mathvariant="normal">∇</mi><mi>θ</mi></msub><mi>J</mi><mo stretchy="false">(</mo><msub><mi>V</mi><mi>θ</mi></msub><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}\mathbf{e}_t = (1-\lambda)\sum_{k}^t \lambda^{t-k} \nabla_\theta V_\theta, \;\;\;\; \mu_t = (1-\beta)\sum_{k}^t \beta^{t-k} \nabla_\theta J(V_\theta)\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:3.382674em;vertical-align:-1.441337em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.941337em;"><span style="top:-3.941337em;"><span class="pstrut" style="height:3.780561em;"></span><span class="mord"><span class="mord"><span class="mord"><span class="mord mathbf">e</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal">λ</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.780561em;"><span style="top:-1.8478869999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.300005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.302113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">λ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal">μ</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.2805559999999999em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop op-limits"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.780561em;"><span style="top:-1.8478869999999998em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span><span style="top:-3.0500049999999996em;"><span class="pstrut" style="height:3.05em;"></span><span><span class="mop op-symbol large-op">∑</span></span></span><span style="top:-4.300005em;margin-left:0em;"><span class="pstrut" style="height:3.05em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">t</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.302113em;"><span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8991079999999999em;"><span style="top:-3.113em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mathnormal mtight">t</span><span class="mbin mtight">−</span><span class="mord mathnormal mtight" style="margin-right:0.03148em;">k</span></span></span></span></span></span></span></span></span><span class="mord"><span class="mord">∇</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord mathnormal" style="margin-right:0.09618em;">J</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:-0.22222em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.441337em;"><span></span></span></span></span></span></span></span></span></span></span></span><br/>
Our method also has similarities with residual gradient methods (<a class="tooltip" href="https://scholar.google.com/scholar?q=Residual+algorithms:+Reinforcement+learning+with+function+approximation&btnG="><span>Leemon Baird<br/><i>Residual algorithms: Reinforcement learning with function approximation</i>, 1995</span>Leemon Baird, 1995</a>). A recent example of this is the work of <a class="tooltip" href="https://scholar.google.com/scholar?q=Deep+Residual+Reinforcement+Learning&btnG="><span>Shangtong Zhang, Wendelin Boehmer, Shimon Whiteson<br/><i>Deep Residual Reinforcement Learning</i>, 2019</span>Shangtong Zhang et al. (2019)</a>, who adapt the residual gradient for deep neural networks. Residual methods learn by taking the gradient of the TD loss with respect to both the current value and the next state value <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi><mo stretchy="false">(</mo><msup><mi>S</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">V(S&#x27;)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.001892em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>, but this comes at the cost of requiring two independent samples of <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>S</mi><mo mathvariant="normal" lspace="0em" rspace="0em">′</mo></msup></mrow><annotation encoding="application/x-tex">S&#x27;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.751892em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.05764em;">S</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.751892em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight"><span class="mord mtight">′</span></span></span></span></span></span></span></span></span></span></span></span> (except in deterministic environments). <br/>
Similarly, our work is related to the ``Gradient TD'' family of methods (<a class="tooltip" href="https://scholar.google.com/scholar?q=A+convergent+O+(n)+algorithm+for+off-policy+temporal-difference+learning+with+linear+function+approximation&btnG="><span>Richard S Sutton, Csaba Szepesvári, Hamid Reza Maei<br/><i>A convergent O (n) algorithm for off-policy temporal-difference learning with linear function approximation</i>, 2008</span>Richard S Sutton et al., 2008</a>; <a class="tooltip" href="https://scholar.google.com/scholar?q=Fast+gradient-descent+methods+for+temporal-difference+learning+with+linear+function+approximation&btnG="><span>Richard S Sutton, Hamid Reza Maei, Doina Precup, Shalabh Bhatnagar, David Silver, Csaba Szepesvári, Eric Wiewiora<br/><i>Fast gradient-descent methods for temporal-difference learning with linear function approximation</i>, 2009</span>Richard S Sutton et al., 2009</a>). These methods attempt to maintain an expectation (over states) of the TD update, which allows to directly optimize the Bellman objective. While the exact relationship between GTD and ``momentum TD'' is not known, they both attempt to maintain an ``expected update'' and adjust parameters according to it; the first approximates the one-step linear TD solution, while the latter approximates the one-step batch TD update. Note that linear GTD methods can also be accelerated with momentum-style updates (<a class="tooltip" href="https://scholar.google.com/scholar?q=Accelerated+gradient+temporal+difference+learning+algorithms&btnG="><span>Dominik Meyer, Rémy Degenne, Ahmed Omrane, Hao Shen<br/><i>Accelerated gradient temporal difference learning algorithms</i>, 2014</span>Dominik Meyer et al., 2014</a>), low-rank approximations for part of the Hessian (<a class="tooltip" href="https://scholar.google.com/scholar?q=Accelerated+gradient+temporal+difference+learning&btnG="><span>Yangchen Pan, Adam White, Martha White<br/><i>Accelerated gradient temporal difference learning</i>, 2016</span>Yangchen Pan et al., 2016</a>), and adaptive learning rates (<a class="tooltip" href="https://scholar.google.com/scholar?q=Finite-time+performance+bounds+and+adaptive+learning+rate+selection+for+two+time-scale+reinforcement+learning&btnG="><span>Harsh Gupta, R Srikant, Lei Ying<br/><i>Finite-time performance bounds and adaptive learning rate selection for two time-scale reinforcement learning</i>, 2019</span>Harsh Gupta et al., 2019</a>). <br/>

More directly related to this work is that of <a class="tooltip" href="https://scholar.google.com/scholar?q=Adaptive+Temporal+Difference+Learning+with+Linear+Function+Approximation&btnG="><span>Tao Sun, Han Shen, Tianyi Chen, Dongsheng Li<br/><i>Adaptive Temporal Difference Learning with Linear Function Approximation</i>, 2020</span>Tao Sun et al. (2020)</a>, who show convergence properties of a rescaled momentum for linear TD(0). While most (if not every) deep reinforcement learning method implicitly uses some form of momentum and/or adaptive learning rate as part of the deep learning toolkit, <a class="tooltip" href="https://scholar.google.com/scholar?q=Adaptive+Temporal+Difference+Learning+with+Linear+Function+Approximation&btnG="><span>Tao Sun, Han Shen, Tianyi Chen, Dongsheng Li<br/><i>Adaptive Temporal Difference Learning with Linear Function Approximation</i>, 2020</span>Tao Sun et al. (2020)</a> properly analyse the use of momentum in a (linear) TD context. <a class="tooltip" href="https://scholar.google.com/scholar?q=Applicability+of+Momentum+in+the+Methods+of+Temporal+Learning&btnG="><span>Dhawal Gupta<br/><i>Applicability of Momentum in the Methods of Temporal Learning</i>, 2020</span>Dhawal Gupta (2020)</a> also analyses momentum in the context of a linear TD(0) and TD(<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>λ</mi></mrow><annotation encoding="application/x-tex">\lambda</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">λ</span></span></span></span>), with surprising negative results suggesting naively applying momentum may hurt stability and convergence in minimal MDPs.<br/>
Another TD-aware adaptive method is that of <a class="tooltip" href="https://scholar.google.com/scholar?q=TDprop:+Does+Jacobi+Preconditioning+Help+Temporal+Difference+Learning?&btnG="><span>Joshua Romoff, Peter Henderson, David Kanaa, Emmanuel Bengio, Ahmed Touati, Pierre-Luc Bacon, Joelle Pineau<br/><i>TDprop: Does Jacobi Preconditioning Help Temporal Difference Learning?</i>, 2020</span>Joshua Romoff et al. (2020)</a>, who derive per-parameter adaptive learning rates, reminiscent of RMSProp (<a class="tooltip" href="https://scholar.google.com/scholar?q=Neural+networks+for+machine+learning+lecture+6a+overview+of+mini-batch+gradient+descent&btnG="><span>Geoffrey Hinton, Nitish Srivastava, Kevin Swersky<br/><i>Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</i>, 2012</span>Geoffrey Hinton et al., 2012</a>), by considering a (diagonal) Jacobi preconditioning that takes into account the bootstrap term in TD. <br/>
Finally, we note that, as far as we know, recent deep RL works all use some form of adaptive gradient method, Adam (<a class="tooltip" href="https://scholar.google.com/scholar?q=Adam:+a+method+for+stochastic+optimization+(2014)&btnG="><span>Diederik Kingma, Jimmy Ba<br/><i>Adam: a method for stochastic optimization (2014)</i>, 2015</span>Diederik Kingma et al., 2015</a>) being an optimizer of choice, closely followed by RMSProp (<a class="tooltip" href="https://scholar.google.com/scholar?q=Neural+networks+for+machine+learning+lecture+6a+overview+of+mini-batch+gradient+descent&btnG="><span>Geoffrey Hinton, Nitish Srivastava, Kevin Swersky<br/><i>Neural networks for machine learning lecture 6a overview of mini-batch gradient descent</i>, 2012</span>Geoffrey Hinton et al., 2012</a>); notable examples of such works include those of <a class="tooltip" href="https://scholar.google.com/scholar?q=Playing+atari+with+deep+reinforcement+learning&btnG="><span>Volodymyr Mnih, Koray Kavukcuoglu, David Silver, Alex Graves, Ioannis Antonoglou, Daan Wierstra, Martin Riedmiller<br/><i>Playing atari with deep reinforcement learning</i>, 2013</span>Volodymyr Mnih et al. (2013)</a>, <a class="tooltip" href="https://scholar.google.com/scholar?q=Proximal+policy+optimization+algorithms&btnG="><span>John Schulman, Filip Wolski, Prafulla Dhariwal, Alec Radford, Oleg Klimov<br/><i>Proximal policy optimization algorithms</i>, 2017</span>John Schulman et al. (2017)</a>, <a class="tooltip" href="https://scholar.google.com/scholar?q=Rainbow:+Combining+improvements+in+deep+reinforcement+learning&btnG="><span>Matteo Hessel, Joseph Modayil, Hado Van Hasselt, Tom Schaul, Georg Ostrovski, Will Dabney, Dan Horgan, Bilal Piot, Mohammad Azar, David Silver<br/><i>Rainbow: Combining improvements in deep reinforcement learning</i>, 2018</span>Matteo Hessel et al. (2018)</a>, and <a class="tooltip" href="https://openreview.net/forum?id=r1lyTjAqYX"><span>Steven Kapturowski, Georg Ostrovski, Will Dabney, John Quan, Remi Munos<br/><i>Recurrent Experience Replay in Distributed Reinforcement Learning</i>, 2019</span>Steven Kapturowski et al. (2019)</a>. We also note the work of <a class="tooltip" href="https://scholar.google.com/scholar?q=Performance+comparison+of+different+momentum+techniques+on+deep+reinforcement+learning&btnG="><span>M. Sarigül, M. Avci<br/><i>Performance comparison of different momentum techniques on deep reinforcement learning</i>, 2018</span>M. Sarigül et al. (2018)</a>, comparing various SGD variants on the game of Othello, showing significant differences based on the choice of optimizer.<br/>
<b>On Taylor approximations</b> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>  </mtext><mtext>  </mtext></mrow><annotation encoding="application/x-tex">\;\;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span></span></span> <a class="tooltip" href="https://arxiv.org/abs/1611.02345"><span>David Balduzzi, Brian McWilliams, Tony Butler-Yeoman<br/><i>Neural taylor approximations: Convergence and exploration in rectifier networks</i>, 2017</span>David Balduzzi et al. (2017)</a> note that while theory suggests that Taylor expansions around parameters should not be useful because of the "non-convexity" of ReLU neural networks, there nonetheless exists local regions in parameter space where the Taylor expansion is consistent. Much earlier work by <a class="tooltip" href="https://scholar.google.com/scholar?q=Using+the+Taylor+expansion+of+multilayer+feedforward+neural+networks&btnG="><span>Andries Petrus Engelbrecht<br/><i>Using the Taylor expansion of multilayer feedforward neural networks</i>, 2000</span>Andries Petrus Engelbrecht (2000)</a> also suggests that Taylor expansions of small sigmoid neural networks are easier to optimize. 
Using Taylor approximations around parameters to find how to prune neural networks also appears to be an effective approach with a long history (<a class="tooltip" href="https://scholar.google.com/scholar?q=Optimal+brain+damage&btnG="><span>Yann LeCun, John S Denker, Sara A Solla<br/><i>Optimal brain damage</i>, 1990</span>Yann LeCun et al., 1990</a>; <a class="tooltip" href="https://scholar.google.com/scholar?q=Second+order+derivatives+for+network+pruning:+Optimal+brain+surgeon&btnG="><span>Babak Hassibi, David G Stork<br/><i>Second order derivatives for network pruning: Optimal brain surgeon</i>, 1993</span>Babak Hassibi et al., 1993</a>; <a class="tooltip" href="https://scholar.google.com/scholar?q=A+new+pruning+heuristic+based+on+variance+analysis+of+sensitivity+information&btnG="><span>Andries Petrus Engelbrecht<br/><i>A new pruning heuristic based on variance analysis of sensitivity information</i>, 2001</span>Andries Petrus Engelbrecht, 2001</a>; <a class="tooltip" href="https://scholar.google.com/scholar?q=Pruning+convolutional+neural+networks+for+resource+efficient+inference&btnG="><span>Pavlo Molchanov, Stephen Tyree, Tero Karras, Timo Aila, Jan Kautz<br/><i>Pruning convolutional neural networks for resource efficient inference</i>, 2016</span>Pavlo Molchanov et al., 2016</a>).<br/>

<b>On policy-gradient methods and others</b> <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext>  </mtext><mtext>  </mtext></mrow><annotation encoding="application/x-tex">\;\;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0em;vertical-align:0em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span></span></span> While not discussed in this paper, another class of methods used to solve RL problems are PG methods. They consist in taking gradients of the objective wrt a directly parameterized policy (rather than inducing policies from value functions). We note in particular the work of <a class="tooltip" href="https://scholar.google.com/scholar?q=Infinite-horizon+policy-gradient+estimation&btnG="><span>Jonathan Baxter, Peter L Bartlett<br/><i>Infinite-horizon policy-gradient estimation</i>, 2001</span>Jonathan Baxter et al. (2001)</a>, who analyse the bias of momentum-like cumulated policy gradients (referred to as traces therein), showing that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>β</mi></mrow><annotation encoding="application/x-tex">\beta</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8888799999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span></span></span></span> the momentum parameter should be chosen such that <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mi mathvariant="normal">/</mi><mo stretchy="false">(</mo><mn>1</mn><mo>−</mo><mi>β</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">1/(1-\beta)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord">1</span><span class="mord">/</span><span class="mopen">(</span><span class="mord">1</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span><span class="mbin">−</span><span class="mspace" style="margin-right:0.2222222222222222em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.05278em;">β</span><span class="mclose">)</span></span></span></span> exceeds the mixing time of the MDP.<br/>
Let us also note the method of <a class="tooltip" href="https://scholar.google.com/scholar?q=Momentum+in+reinforcement+learning&btnG="><span>Nino Vieillard, Bruno Scherrer, Olivier Pietquin, Matthieu Geist<br/><i>Momentum in reinforcement learning</i>, 2020</span>Nino Vieillard et al. (2020)</a>, Momentum Value Iteration, which uses the concept of an exponential moving average objective for a decoupled (with its own parameters) action-value function from which the greedy policy being evaluated is induced. This moving average is therein referred to as <i>momentum</i>; even though it is not properly speaking the optimizational acceleration of <a class="tooltip" href="https://vsokolov.org/courses/750/2018/files/polyak64.pdf"><span>Boris T Polyak<br/><i>Some methods of speeding up the convergence of iteration methods</i>, 1964</span>Boris T Polyak (1964)</a>, its form is similar.<br/>
<br/>
<a name="s22"></a><h3>8 ICML 2020</h3><br/>
<a name="s23"></a><h4>8.1 Generalization in Deep RL</h4><br/>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Discount+Factor+as+a+Regularizer+in+Reinforcement+Learning&btnG="><span>Ron Amit, Ron Meir, Kamil Ciosek<br/><i>Discount Factor as a Regularizer in Reinforcement Learning</i>, 2020</span>Ron Amit et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6021">[poster]</a></sup>, in TD reducing gamma is equivalent to adding a particular kind of regularizer, this is done by tweaking the TD update a bit, in particular penalizing V(s)^2 factored by some gamma-dependent scale
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Evaluating+the+Performance+of+Reinforcement+Learning+Algorithms&btnG="><span>Scott Jordan, Yash Chandak, Daniel Cohen, Mengxue Zhang, Philip Thomas<br/><i>Evaluating the Performance of Reinforcement Learning Algorithms</i>, 2020</span>Scott Jordan et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6301">[poster]</a></sup></li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Representations+for+Stable+Off-Policy+Reinforcement+Learning&btnG="><span>Dibya Ghosh, Marc G Bellemare<br/><i>Representations for Stable Off-Policy Reinforcement Learning</i>, 2020</span>Dibya Ghosh et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6666">[poster]</a></sup></li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Gradient+Temporal-Difference+Learning+with+Regularized+Corrections&btnG="><span>Sina Ghiassian, Andrew Patterson, Shivam Garg, Dhawal Gupta, Adam White, Martha White<br/><i>Gradient Temporal-Difference Learning with Regularized Corrections</i>, 2020</span>Sina Ghiassian et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6067">[poster]</a></sup></li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Inferring+DQN+structure+for+high-dimensional+continuous+control&btnG="><span>Andrey Sakryukin, Chedy Raïssi, Mohan Kankanhalli<br/><i>Inferring DQN structure for high-dimensional continuous control</i>, 2020</span>Andrey Sakryukin et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6740">[poster]</a></sup></li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+the+Expressivity+of+Neural+Networks+for+Deep+Reinforcement+Learning&btnG="><span>Kefan Dong, Yuping Luo, Tengyu Ma<br/><i>On the Expressivity of Neural Networks for Deep Reinforcement Learning</i>, 2019</span>Kefan Dong et al. (2019)</a><sup><a href="https://icml.cc/virtual/2020/poster/6381">[poster]</a></sup>, example MDP+policy where the dynamics can be learned with 6 linear regions by a neural net while the Q function would take thousands of linear regions. Confirm these intuitions on Mujoco, MB can do better.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Context-aware+Dynamics+Model+for+Generalization+in+Model-Based+Reinforcement+Learning&btnG="><span>Kimin Lee, Younggyo Seo, Seunghyun Lee, Honglak Lee, Jinwoo Shin<br/><i>Context-aware Dynamics Model for Generalization in Model-Based Reinforcement Learning</i>, 2020</span>Kimin Lee et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6363">[poster]</a></sup>, training a context with backward+forward predictions as a task and condition model-based or model-free on the context helps with generalization and task meta-parameters extrapolation. The context is learned independently it seems, perhaps this has value instead of doing end-to-end? 
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=What+Can+Learned+Intrinsic+Rewards+Capture?&btnG="><span>Zeyu Zheng, Junhyuk Oh, Matteo Hessel, Zhongwen Xu, Manuel Kroiss, Hado Van Hasselt, David Silver, Satinder Singh<br/><i>What Can Learned Intrinsic Rewards Capture?</i>, 2020</span>Zeyu Zheng et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6150">[poster]</a></sup>, meta-learn intrinsic rewards from several agent lifetimes, test on simple environments to see what kind of rewards are learned depending on the structure of the environment. Knowledge captures some regularities of the MDPs, “what to do” instead of “how to do”
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Can+Increasing+Input+Dimensionality+Improve+Deep+Reinforcement+Learning?&btnG="><span>Kei Ota, Tomoaki Oiki, Devesh K Jha, Toshisada Mariyama, Daniel Nikovski<br/><i>Can Increasing Input Dimensionality Improve Deep Reinforcement Learning?</i>, 2020</span>Kei Ota et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6387">[poster]</a></sup></li></ul><br/>

<a name="s24"></a><h4>8.2 DNN generalization</h4>
<ul><li><a href=https://icml.cc/virtual/2020/poster/6013>https://icml.cc/virtual/2020/poster/6013</a> Double Trouble in Double Descent: Bias and Variance(s) in the Lazy Regime, in the lazy regime, as you increase capacity, weight init variance goes to 0, bias and sample variance is well behaved and remains constant after some threshold (the phase transition?).
</li><li><a href=https://icml.cc/virtual/2020/poster/5879>https://icml.cc/virtual/2020/poster/5879</a>  Being Bayesian, Even Just a Bit, Fixes Overconfidence in ReLU Networks, cool trick, take a MAP trained network, re-fit last layer’s weights as a gaussian to “be a bit bayesian”, and you get good estimates away from the data. (one downside is they only seem to test this for ridiculous perturbations, not for OoD but real data like SHVN <-> CIFAR).
</li><li><a href=https://icml.cc/virtual/2020/poster/6065>https://icml.cc/virtual/2020/poster/6065</a>  SDE-Net: Equipping Deep Neural Networks with Uncertainty Estimates, get epistemic uncertainty by training a part of the neural network to do brownian motion diffusion, low diffusion on training set, high diffusion on OoD. Also get aleatoric uncertainty with entropy/gaussian variance prediction
</li><li><a href=https://icml.cc/virtual/2020/poster/5889>https://icml.cc/virtual/2020/poster/5889</a>  Optimizing Data Usage via Differentiable Rewards, that’s actually clever, active learning style where the “reward” (loss) of the teacher is the gradient alignment between the samples shown to the student and the samples of the (meta?)test set. That reward is the derivative of the performance on the test set of the student wrt the teacher’s choices, thus the title.
</li><li><a href=https://icml.cc/virtual/2020/poster/6211>https://icml.cc/virtual/2020/poster/6211</a>  The Implicit Regularization of Stochastic Gradient Flow for Least Squares
</li><li><a href=https://icml.cc/virtual/2020/poster/6812>https://icml.cc/virtual/2020/poster/6812</a>  Is Local SGD Better than Minibatch SGD?
</li><li><a href=https://icml.cc/virtual/2020/poster/6356>https://icml.cc/virtual/2020/poster/6356</a>  Neural Kernels Without Tangents, explicit convolutional kernel with nice properties, works on CIFAR10, still a kernel, still quadratic.
</li><li><a href=https://icml.cc/virtual/2020/poster/6207>https://icml.cc/virtual/2020/poster/6207</a>  Landscape Connectivity and Dropout Stability of SGD Solutions for Over-parameterized Neural Networks, “As neural networks grow wider the solutions obtained by SGD become increasingly more dropout stable and barriers between local optima disappear.”
</li><li><a href=https://icml.cc/virtual/2020/poster/6004>https://icml.cc/virtual/2020/poster/6004</a>  Training Neural Networks for and by Interpolation
</li><li><a href=https://icml.cc/virtual/2020/poster/6488>https://icml.cc/virtual/2020/poster/6488</a>  Weakly-Supervised Disentanglement Without Compromises,
</li><li><a href=https://icml.cc/virtual/2020/poster/6208>https://icml.cc/virtual/2020/poster/6208</a>  Learning Representations that Support Extrapolation, so like Batch Norm, but instead it’s context norm, and it allows analogies by normalizing away irrelevant dimensions of the objects used in the analogy. This is fairly specific to the task and I don’t see how this is generalizable. One would say, hackish :3 but who knows.
</li><li><a href=https://icml.cc/virtual/2020/poster/5788>https://icml.cc/virtual/2020/poster/5788</a>  Hybrid Stochastic-Deterministic Minibatch Proximal Gradient: Less-Than-Single-Pass Optimization with Nearly Optimal Generalization
</li><li><a href=https://icml.cc/virtual/2020/poster/6617>https://icml.cc/virtual/2020/poster/6617</a>  Generalization via Derandomization (Dan Roy)
</li><li><a href=https://icml.cc/virtual/2020/poster/6246>https://icml.cc/virtual/2020/poster/6246</a>  Improving generalization by controlling label-noise information in neural network weights
</li><li><a href=https://icml.cc/virtual/2020/poster/5960>https://icml.cc/virtual/2020/poster/5960</a>  Unique Properties of Wide Minima in Deep Networks
</li><li><a href=https://icml.cc/virtual/2020/poster/6307>https://icml.cc/virtual/2020/poster/6307</a>  On the Noisy Gradient Descent that Generalizes as SGD
</li><li><a href=https://icml.cc/virtual/2020/poster/6384>https://icml.cc/virtual/2020/poster/6384</a>  Why bigger is not always better: on finite and infinite neural networks
</li><li><a href=https://icml.cc/virtual/2020/poster/6763>https://icml.cc/virtual/2020/poster/6763</a>  The Implicit and Explicit Regularization Effects of Dropout, derive the effects of dropout as 2 regularization terms, these involve Hessians so more expensive but for two NLP tasks appears to improve generalization somewhat.
</li><li><a href=https://icml.cc/virtual/2020/poster/6678>https://icml.cc/virtual/2020/poster/6678</a>  Empirical Study of the Benefits of Overparameterization in Learning Latent Variable Models
</li><li><a href=https://icml.cc/virtual/2020/poster/6710>https://icml.cc/virtual/2020/poster/6710</a>  The Neural Tangent Kernel in High Dimensions: Triple Descent and a Multi-Scale Theory of Generalization
</li><li><a href=https://icml.cc/virtual/2020/poster/6699>https://icml.cc/virtual/2020/poster/6699</a>  Linear Mode Connectivity and the Lottery Ticket Hypothesis
</li><li><a href=https://icml.cc/virtual/2020/poster/6259>https://icml.cc/virtual/2020/poster/6259</a>  Rethinking Bias-Variance Trade-off for Generalization of Neural Networks
</li><li><a href=https://icml.cc/virtual/2020/poster/5884>https://icml.cc/virtual/2020/poster/5884</a>  Disentangling Trainability and Generalization in Deep Neural Networks
</li><li><a href=https://icml.cc/virtual/2020/poster/6144>https://icml.cc/virtual/2020/poster/6144</a>  The Natural Lottery Ticket Winner: Reinforcement Learning with Ordinary Neural Circuits
</li><li><a href=https://icml.cc/virtual/2020/poster/6129>https://icml.cc/virtual/2020/poster/6129</a>  Proving the Lottery Ticket Hypothesis: Pruning is All You Need, title, proof of the hypothesis, given some network you can approximate it with high probability with net of width poly(depth, width, #input, 1/eps, log(1/delta)) and depth 2*depth.
</li><li><a href=https://icml.cc/virtual/2020/poster/5808>https://icml.cc/virtual/2020/poster/5808</a>  Rigging the Lottery: Making All Tickets Winners, start with random mask, train a bit, kill low weights, unmask new weights, repeat. Also, with lotteries there is no longer a piecewise linear (bezier) optimal path between minima, it’s almost flat but not quite, small bumps suggesting some sort of basin is found. RiGL the proposed method seems to solve that.
</li><li><a href=https://icml.cc/virtual/2020/poster/6230>https://icml.cc/virtual/2020/poster/6230</a>  Momentum Improves Normalized SGD
</li></ul>
  
<a name="s25"></a><h4>8.3 Model-based RL</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Active+world+model+learning+in+agent-rich+environments+with+progress+curiosity&btnG="><span>Kun Ho Kim<br/><i>Active world model learning in agent-rich environments with progress curiosity</i>, 2020</span>Kun Ho Kim (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6064">[poster]</a></sup> new 3d env with static, random, periodic, animate types of observations/other agents. Train a policy whose reward is the -Loss of the world model. To have a proxy reward, keep EMA of old model, difference of predictive performance. Cool results. 
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Selective+Dyna-style+planning+under+limited+model+capacity&btnG="><span>Zaheer Abbas, Samuel Sokota, Erin Talvitie, Martha White<br/><i>Selective Dyna-style planning under limited model capacity</i>, 2020</span>Zaheer Abbas et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6672">[poster]</a></sup>, heteroskedastic regression, learning the “error”/variance, this alone doesn’t help but then planning and weighing simulated transitions inversely by that error helps.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Goal-aware+prediction:+Learning+to+model+what+matters&btnG="><span>Suraj Nair, Silvio Savarese, Chelsea Finn<br/><i>Goal-aware prediction: Learning to model what matters</i>, 2020</span>Suraj Nair et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6271">[poster]</a></sup>, models underfit on complex, diverse scenes. Train world model such that they are more accurate on the best trajectories by conditioning the model on the goal. Generalizes to unseen goals. 
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=A+Game+Theoretic+Framework+for+Model+Based+Reinforcement+Learning&btnG="><span>Aravind Rajeswaran, Igor Mordatch, Vikash Kumar<br/><i>A Game Theoretic Framework for Model Based Reinforcement Learning</i>, 2020</span>Aravind Rajeswaran et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6314">[poster]</a></sup>, think of learning model using policy data + learning opt policy using model as a two-player game, it’s equilibrium solves the MDP. One of the players has to learn slower than the other for it to be stable. Just doing that properly gives order of magnitude gains over sota on continuous control tasks. 
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Sequential+transfer+in+reinforcement+learning+with+a+generative+model&btnG="><span>Andrea Tirinzoni, Riccardo Poiani, Marcello Restelli<br/><i>Sequential transfer in reinforcement learning with a generative model</i>, 2020</span>Andrea Tirinzoni et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6567">[poster]</a></sup>, strategy: spend some time acting to identify the current task, train from a sequence of tasks rather than iid tasks and use their temporal correlations. Model P and R with theta’, and then try to select (s,a) that maximizes mutual information between (theta; theta’) (requires a generative model of P,R). They apply spectral methods to learn the hidden markov model of task transitions.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Improving+Generative+Imagination+in+Object-Centric+World+Models&btnG="><span>Zhixuan Lin, Yi-Fu Wu, Skand Peri, Bofeng Fu, Jindong Jiang, Sungjin Ahn<br/><i>Improving Generative Imagination in Object-Centric World Models</i>, 2020</span>Zhixuan Lin et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6582">[poster]</a></sup>, clever decomposition of objects, attributes, + environment. Interaction is a GNN, “situation awareness” is done with attention on the environment encoding. Their setup allows the distribution of object attributes to be multimodal and generates good distributions of futures.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Model-Based+Reinforcement+Learning+with+Value-Targeted+Regression&btnG="><span>Alex Ayoub, Zeyu Jia, Csaba Szepesvari, Mengdi Wang, Lin F Yang<br/><i>Model-Based Reinforcement Learning with Value-Targeted Regression</i>, 2020</span>Alex Ayoub et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6705">[poster]</a></sup>, rigorous analysis of models that predict V(s_t+1) instead of s_t+1, like MuZero does, also proposes a new tabular algorithm that does things properly
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Bidirectional+Model-based+Policy+Optimization&btnG="><span>Hang Lai, Jian Shen, Weinan Zhang, Yong Yu<br/><i>Bidirectional Model-based Policy Optimization</i>, 2020</span>Hang Lai et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6797">[poster]</a></sup>, learn forward and backward model, also learn a backward policy (either via max likelihood, or via a GAN?). To learn instead of uniform sampling from buffer for s0 they sample propto exp(beta V), the rollout forward and backward and do a PG step. In environment interactions they do N rollouts of H steps and take argmax for a0. There’s some theory and good looking bounds. Experiments on Mujoco.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Planning+to+Explore+via+Self-Supervised+World+Models&btnG="><span>Ramanan Sekar, Oleh Rybkin, Kostas Daniilidis, Pieter Abbeel, Danijar Hafner, Deepak Pathak<br/><i>Planning to Explore via Self-Supervised World Models</i>, 2020</span>Ramanan Sekar et al. (2020)</a><sup><a href="https://icml.cc/virtual/2020/poster/6556">[poster]</a></sup> Plan to explore by rolling out trajectories and predict intrinsic reward, is compatible with any IntR method. They learn ensembles of models to use disagreement of intrinsic reward prediction as where to go. Learns from images of mujoco agents. Presumably can achieve zero-shot if relabeling the buffer with new reward doesn’t require interaction.
</li></ul><br/>

<br/>

<br/>

<br/>

<br/>

<br/>

<br/>

<br/>

<br/>

<br/>

<a name="s26"></a><h3>9 NeurIPS 2020</h3><br/>
<a name="s27"></a><h4>9.1 Deep RL</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Discor:+Corrective+feedback+in+reinforcement+learning+via+distribution+correction&btnG="><span>Aviral Kumar, Abhishek Gupta, Sergey Levine<br/><i>Discor: Corrective feedback in reinforcement learning via distribution correction</i>, 2020</span>Aviral Kumar et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_d7f426ccbc6db7e235c57958c21c5dfa.html">[poster]</a></sup>, ``reweighting samples based on the estimated accuracy of their target values'' prevents bootstrapping from propagating lots of errors. Robotic control, Atari.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=An+Equivalence+between+Loss+Functions+and+Non-Uniform+Sampling+in+Experience+Replay&btnG="><span>Scott Fujimoto, David Meger, Doina Precup<br/><i>An Equivalence between Loss Functions and Non-Uniform Sampling in Experience Replay</i>, 2020</span>Scott Fujimoto et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_a3bf6e4db673b6449c2f7d13ee6ec9c0.html">[poster]</a></sup>, Prioritized Experience Replay is equivalent to a uniform replay sampler with a modified loss. This insight allows to design a better PER.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Robust+Reinforcement+Learning+via+Adversarial+training+with+Langevin+Dynamics&btnG="><span>Parameswaran Kamalaruban, Yu-Ting Huang, Ya-Ping Hsieh, Paul Rolland, Cheng Shi, Volkan Cevher<br/><i>Robust Reinforcement Learning via Adversarial training with Langevin Dynamics</i>, 2020</span>Parameswaran Kamalaruban et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_5cb0e249689cd6d8369c4885435a56c2.html">[poster]</a></sup>, adversarial training is like a two-player game. I guess the adversarial part comes from some minmax formulation, kind of like an agent trying to outdo itself. Not sure I understand really.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=MDP+homomorphic+networks:+Group+symmetries+in+reinforcement+learning&btnG="><span>Elise van der Pol, Daniel Worrall, Herke van Hoof, Frans Oliehoek, Max Welling<br/><i>MDP homomorphic networks: Group symmetries in reinforcement learning</i>, 2020</span>Elise van der Pol et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_2be5f9c2e3620eb73c2972d7552b6cb5.html">[poster]</a></sup>, identify the symmetries of an MDP to learn with less data (it's not clear if the symmetries have to be given or not though).
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Instance-based+Generalization+in+Reinforcement+Learning&btnG="><span>Martin Bertran, Natalia Martinez, Mariano Phielipp, Guillermo Sapiro<br/><i>Instance-based Generalization in Reinforcement Learning</i>, 2020</span>Martin Bertran et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_82674fc29bc0d9895cee346548c2cb5c.html">[poster]</a></sup>, CoinRun, train instance(task)-agnostic policies by training ensembles with no overlap on tasks (I think? there seems to be more to it).
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=One+Solution+is+Not+All+You+Need:+Few-Shot+Extrapolation+via+Structured+MaxEnt+RL&btnG="><span>Saurabh Kumar, Aviral Kumar, Sergey Levine, Chelsea Finn<br/><i>One Solution is Not All You Need: Few-Shot Extrapolation via Structured MaxEnt RL</i>, 2020</span>Saurabh Kumar et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_5d151d1059a6281335a10732fc49620e.html">[poster]</a></sup>, MaxEnt, ``learning diverse behaviors for accomplishing a task can directly lead to behavior that generalizes to varying environments, without needing to perform explicit perturbations during training''
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Direct+policy+gradients:+Direct+optimization+of+policies+in+discrete+action+spaces&btnG="><span>Guy Lorberbom, Chris J Maddison, Nicolas Heess, Tamir Hazan, Daniel Tarlow<br/><i>Direct policy gradients: Direct optimization of policies in discrete action spaces</i>, 2020</span>Guy Lorberbom et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_d1e7b08bdb7783ed4fb10abe92c22ffd.html">[poster]</a></sup>, directed policy gradient, combine A* with direct optimization, prune nodes to get better gradients by knowing the return upper bound. (relevant to mol project?)
</li><li>-- Day 2 --
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Conservative+Q-Learning+for+Offline+Reinforcement+Learning&btnG="><span>Aviral Kumar, Aurick Zhou, George Tucker, Sergey Levine<br/><i>Conservative Q-Learning for Offline Reinforcement Learning</i>, 2020</span>Aviral Kumar et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_0d2b2061826a5df3221116a5085a6052.html">[poster]</a></sup>, learn a pessimistic/conservative <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Q</mi></mrow><annotation encoding="application/x-tex">Q</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.8777699999999999em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">Q</span></span></span></span> function by penalizing big Q values (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mo><mi mathvariant="normal">arg max</mi><mo>⁡</mo></mo><mi>μ</mi></msub><mi>Q</mi><mo stretchy="false">(</mo><mi>s</mi><mo separator="true">,</mo><mi>a</mi><mo stretchy="false">)</mo><mo separator="true">;</mo><mi>a</mi><mo>∼</mo><mi>μ</mi></mrow><annotation encoding="application/x-tex">\argmax_\mu Q(s,a); a \sim \mu</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.130248em;vertical-align:-0.380248em;"></span><span class="mop"><span class="mop"><span class="mord mathrm">a</span><span class="mord mathrm">r</span><span class="mord mathrm" style="margin-right:0.01389em;">g</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathrm">m</span><span class="mord mathrm">a</span><span class="mord mathrm">x</span></span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.057252000000000025em;"><span style="top:-2.4558600000000004em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">μ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.380248em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">Q</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">a</span><span class="mclose">)</span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">a</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∼</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal">μ</span></span></span></span>).
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Improving+Generalization+in+Reinforcement+Learning+with+Mixture+Regularization&btnG="><span>Kaixin Wang, Bingyi Kang, Jie Shao, Jiashi Feng<br/><i>Improving Generalization in Reinforcement Learning with Mixture Regularization</i>, 2020</span>Kaixin Wang et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_5a751d6a0b6ef05cfe51b86e5d1458e6.html">[poster]</a></sup>, input mixup, but from inputs of different tasks. Mixup target and losses as well (V target, pi mixture).
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Reinforcement+Learning+with+Augmented+Data&btnG="><span>Michael Laskin, Kimin Lee, Adam Stooke, Lerrel Pinto, Pieter Abbeel, Aravind Srinivas<br/><i>Reinforcement Learning with Augmented Data</i>, 2020</span>Michael Laskin et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_e615c82aba461681ade82da2da38004a.html">[poster]</a></sup>, 2 more input augmentations for RL, translate and window (crop).
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=What+Did+You+Think+Would+Happen?+Explaining+Agent+Behaviour+through+Intended+Outcomes&btnG="><span>Herman Ho-Man Yau, Chris Russell, Simon Hadfield<br/><i>What Did You Think Would Happen? Explaining Agent Behaviour through Intended Outcomes</i>, 2020</span>Herman Ho-Man Yau et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_d5ab8dc7ef67ca92e41d730982c5c602.html">[poster]</a></sup>, by learning a linear expectation model (on the top layer?) and a reward model, you can check what the model expected to happen.
  </li><li>-- Day 3 --
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Understanding+the+role+of+training+regimes+in+continual+learning&btnG="><span>Seyed Iman Mirzadeh, Mehrdad Farajtabar, Razvan Pascanu, Hassan Ghasemzadeh<br/><i>Understanding the role of training regimes in continual learning</i>, 2020</span>Seyed Iman Mirzadeh et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_518a38cc9a0173d0b2dc088166981cf8.html">[poster]</a></sup>, continual learning, analysis of how inner-loop hyperparameters affect forgetting.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Reward+Propagation+Using+Graph+Convolutional+Networks&btnG="><span>Martin Klissarov, Doina Precup<br/><i>Reward Propagation Using Graph Convolutional Networks</i>, 2020</span>Martin Klissarov et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_970627414218ccff3497cb7a784288f5.html">[poster]</a></sup>, use graph convnet to propage rewards for potential-function-based reward shaping.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Discovering+reinforcement+learning+algorithms&btnG="><span>Junhyuk Oh, Matteo Hessel, Wojciech M Czarnecki, Zhongwen Xu, Hado P van Hasselt, Satinder Singh, David Silver<br/><i>Discovering reinforcement learning algorithms</i>, 2020</span>Junhyuk Oh et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_0b96d81f0494fde5428c7aea243c9157.html">[poster]</a></sup>, meta-learn the update rule by forcing agents to output <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>π</mi><mo separator="true">,</mo><mi>y</mi></mrow><annotation encoding="application/x-tex">\pi,y</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.625em;vertical-align:-0.19444em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="mpunct">,</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">y</span></span></span></span>, both softmaxes, and pushing the learned update rule to perform well on a distribution of environments. Training the update rule on simple environments generalizes (0-shot?) to Atari (although sub-sota).
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Meta-gradient+reinforcement+learning+with+an+objective+discovered+online&btnG="><span>Zhongwen Xu, Hado P van Hasselt, Matteo Hessel, Junhyuk Oh, Satinder Singh, David Silver<br/><i>Meta-gradient reinforcement learning with an objective discovered online</i>, 2020</span>Zhongwen Xu et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_ae3d525daf92cee0003a7f2d92c34ea3.html">[poster]</a></sup>, similar to the above, but meta-learn a value target.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Munchausen+reinforcement+learning&btnG="><span>Nino Vieillard, Olivier Pietquin, Matthieu Geist<br/><i>Munchausen reinforcement learning</i>, 2020</span>Nino Vieillard et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_2c6a0bae0f071cbbf0bb3d5b11d90a82.html">[poster]</a></sup>, augment reward with <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>α</mi><mi>log</mi><mo>⁡</mo><mi>π</mi><mo stretchy="false">(</mo><mi>a</mi><mi mathvariant="normal">∣</mi><mi>s</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">\alpha \log \pi(a|s)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.0037em;">α</span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.03588em;">π</span><span class="mopen">(</span><span class="mord mathnormal">a</span><span class="mord">∣</span><span class="mord mathnormal">s</span><span class="mclose">)</span></span></span></span>, makes DQN very good. It implicitly does MaxEnt!
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Predictive+information+accelerates+learning+in+rl&btnG="><span>Kuang-Huei Lee, Ian Fischer, Anthony Liu, Yijie Guo, Honglak Lee, John Canny, Sergio Guadarrama<br/><i>Predictive information accelerates learning in rl</i>, 2020</span>Kuang-Huei Lee et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_89b9e0a6f6d1505fe13dea0f18a2dcfa.html">[poster]</a></sup>, predict repr that maximises mutual information between future and past, accelerates SAC. Contrastive version of Conditional Entropy Bottleneck.
</li></ul><br/>
<a name="s28"></a><h4>9.2 RL Theory/Linear-land</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=A+maximum-entropy+approach+to+off-policy+evaluation+in+average-reward+MDPs&btnG="><span>Nevena Lazic, Dong Yin, Mehrdad Farajtabar, Nir Levine, Dilan Gorur, Chris Harris, Dale Schuurmans<br/><i>A maximum-entropy approach to off-policy evaluation in average-reward MDPs</i>, 2020</span>Nevena Lazic et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_9308b0d6e5898366a4a986bc33f3d3e7.html">[poster]</a></sup>, off-policy evaluation in average reward linear transition setting, ``finding the maximum-entropy distribution subject to matching feature expectations under empirical dynamics'', which matches maxent in supervised learning.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+the+theory+of+transfer+learning:+The+importance+of+task+diversity&btnG="><span>Nilesh Tripuraneni, Michael Jordan, Chi Jin<br/><i>On the theory of transfer learning: The importance of task diversity</i>, 2020</span>Nilesh Tripuraneni et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_59587bffec1c7846f3e34230141556ae.html">[poster]</a></sup>, some new proposed notion of task diversity for tasks with a shared (learned?) representation.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=An+operator+view+of+policy+gradient+methods&btnG="><span>Dibya Ghosh, Marlos C Machado, Nicolas Le Roux<br/><i>An operator view of policy gradient methods</i>, 2020</span>Dibya Ghosh et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_22eda830d1051274a2581d6466c06e6c.html">[poster]</a></sup>, proposes to view PG methods through the operator framework as a combined two operators (improve, project). This allows us to look at those operators and improve them indivudually or consider their interaction. Link with Bellman operator makes sense. Discussion of PPO and friends.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Sample+Efficient+Reinforcement+Learning+via+Low-Rank+Matrix+Estimation&btnG="><span>Devavrat Shah, Dogyoon Song, Zhi Xu, Yuzhe Yang<br/><i>Sample Efficient Reinforcement Learning via Low-Rank Matrix Estimation</i>, 2020</span>Devavrat Shah et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_8d2355364e9a2ba1f82f975414937b43.html">[poster]</a></sup>, when the true Q-function is low-rank, one can learn a low-rank Q much faster. lol, same but for DNNs instead of just linear: <a class="tooltip" href="https://scholar.google.com/scholar?q=Flambe:+Structural+complexity+and+representation+learning+of+low+rank+mdps&btnG="><span>Alekh Agarwal, Sham Kakade, Akshay Krishnamurthy, Wen Sun<br/><i>Flambe: Structural complexity and representation learning of low rank mdps</i>, 2020</span>Alekh Agarwal et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_e894d787e2fd6c133af47140aa156f00.html">[poster]</a></sup>.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Escaping+the+Gravitational+Pull+of+Softmax&btnG="><span>Jincheng Mei, Chenjun Xiao, Bo Dai, Lihong Li, Csaba Szepesvári, Dale Schuurmans<br/><i>Escaping the Gravitational Pull of Softmax</i>, 2020</span>Jincheng Mei et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_f1cf2a082126bf02de0b307778ce73a7.html">[poster]</a></sup>, alternative to softmax, escort
  <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>s</mi><mo stretchy="false">)</mo><mo>=</mo><mfrac><mrow><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>i</mi></msub><msup><mi mathvariant="normal">∣</mi><mi>p</mi></msup></mrow><mrow><munder><mo>∑</mo><mi>j</mi></munder><mi mathvariant="normal">∣</mi><msub><mi>s</mi><mi>j</mi></msub><msup><mi mathvariant="normal">∣</mi><mi>p</mi></msup></mrow></mfrac><mo separator="true">;</mo><mtext>  </mtext><mtext>  </mtext><mi>p</mi><mo>≥</mo><mn>1</mn></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}f(s) = \frac{|s_i|^p}{\sum_j |s_j|^p}; \;\; p\geq 1\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:2.8488180000000005em;vertical-align:-1.174409em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.6744090000000005em;"><span style="top:-3.674409em;"><span class="pstrut" style="height:3.427em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">s</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord"><span class="mopen nulldelimiter"></span><span class="mfrac"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1.427em;"><span style="top:-2.314em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mop"><span class="mop op-symbol small-op" style="position:relative;top:-0.0000050000000000050004em;">∑</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.16195399999999993em;"><span style="top:-2.40029em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.43581800000000004em;"><span></span></span></span></span></span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.311664em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.05724em;">j</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.286108em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.590392em;"><span style="top:-2.9890000000000003em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span></span></span></span></span></span></span><span style="top:-3.23em;"><span class="pstrut" style="height:3em;"></span><span class="frac-line" style="border-bottom-width:0.04em;"></span></span><span style="top:-3.677em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord">∣</span><span class="mord"><span class="mord mathnormal">s</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.31166399999999994em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">i</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mord"><span class="mord">∣</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.664392em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight">p</span></span></span></span></span></span></span></span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.1218180000000002em;"><span></span></span></span></span></span><span class="mclose nulldelimiter"></span></span><span class="mpunct">;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal">p</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">≥</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord">1</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:1.174409em;"><span></span></span></span></span></span></span></span></span></span></span></span>
  works for RL as well to parameterize policies, avoids saturation pitfalls of softmax, and particular pitfalls of PG in RL (entropy reg?).
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Provably+Efficient+Neural+GTD+for+Off-Policy+Learning&btnG="><span>Hoi-To Wai, Zhuoran Yang, Zhaoran Wang, Mingyi Hong<br/><i>Provably Efficient Neural GTD for Off-Policy Learning</i>, 2020</span>Hoi-To Wai et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_75ebb02f92fc30a8040bbd625af999f1.html">[poster]</a></sup>, provably efficient Neural GTD for off-policy learning, with some kind of momentum shenanigan!
</li><li><a class="tooltip" href="https://proceedings.neurips.cc/paper/2020/file/4a5cfa9281924139db466a8a19291aff-Paper.pdf"><span>Zheng Wen, Doina Precup, Morteza Ibrahimi, Andre Barreto, Benjamin Van Roy, Satinder Singh<br/><i>On Efficiency in Hierarchical Reinforcement Learning</i>, 2020</span>Zheng Wen et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_4a5cfa9281924139db466a8a19291aff.html">[poster]</a></sup>, MDPs that have repeated substructures can be leveraged by hierarchical RL.
</li><li>-- Day 2 --
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Is+Long+Horizon+RL+More+Difficult+Than+Short+Horizon+RL?&btnG="><span>Ruosong Wang, Simon S Du, Lin Yang, Sham Kakade<br/><i>Is Long Horizon RL More Difficult Than Short Horizon RL?</i>, 2020</span>Ruosong Wang et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_6734fa703f6633ab896eecbdfad8953a.html">[poster]</a></sup>, Is Long Horizon RL More Difficult Than Short Horizon RL?. No, for tabular episodic learning, difficulty scales log(horizon) rather than the previously conjectured poly(horizon).
  
</li></ul><br/>
<a name="s29"></a><h4>9.3 Model-based RL</h4><br/>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Novelty+Search+in+representational+space+for+sample+efficient+exploration&btnG="><span>Ruo Yu Tao, Vincent François-Lavet, Joelle Pineau<br/><i>Novelty Search in representational space for sample efficient exploration</i>, 2020</span>Ruo Yu Tao et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_5ca41a86596a5ed567d15af0be224952.html">[poster]</a></sup>, Model-based search from instrinsic reward based on distance in encoding space (DNN) with previously seen states. +Information Bottleneck objective.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Breaking+the+sample+size+barrier+in+model-based+reinforcement+learning+with+a+generative+model&btnG="><span>Gen Li, Yuting Wei, Yuejie Chi, Yuantao Gu, Yuxin Chen<br/><i>Breaking the sample size barrier in model-based reinforcement learning with a generative model</i>, 2020</span>Gen Li et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_96ea64f3a1aa2fd00c72faacf0cb8ac9.html">[poster]</a></sup>, theory, with a perturbed model-based planner can reach theoretical lower-bound of the sample size requirement.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=MOReL:+Model-Based+Offline+Reinforcement+Learning&btnG="><span>Rahul Kidambi, Aravind Rajeswaran, Praneeth Netrapalli, Thorsten Joachims<br/><i>MOReL: Model-Based Offline Reinforcement Learning</i>, 2020</span>Rahul Kidambi et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_f7efa4f864ae9b88d43527f4b14f750f.html">[poster]</a></sup>, MOReL, offline batch RL, learn pessimistic MDP from data (value of any policy is a lower bound of true value) and plan/learn policy with it. The MDP transfers to an absorbing low reward state when uncertainy is too high. Mujoco.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=The+Value+Equivalence+Principle+for+Model-Based+Reinforcement+Learning&btnG="><span>Christopher Grimm, André Barreto, Satinder Singh, David Silver<br/><i>The Value Equivalence Principle for Model-Based Reinforcement Learning</i>, 2020</span>Christopher Grimm et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_3bb585ea00014b0e3ebe4c6dd165a358.html">[poster]</a></sup>, ``two models are value equivalent with respect to a set of functions and policies if they yield the same Bellman updates'', we can use this to find good models rather than e.g. likelihood. This is the principle underlying Predictron, VIN, VPN, TreeQN, MuZero.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=The+LoCA+Regret:+A+Consistent+Metric+to+Evaluate+Model-Based+Behavior+in+Reinforcement+Learning&btnG="><span>Harm Van Seijen, Hadi Nekoei, Evan Racah, Sarath Chandar<br/><i>The LoCA Regret: A Consistent Metric to Evaluate Model-Based Behavior in Reinforcement Learning</i>, 2020</span>Harm Van Seijen et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_48db71587df6c7c442e5b76cc723169a.html">[poster]</a></sup>, LoCA Regret, measure robustness of models to local changes in environments. Authors argue this is how we should evaluate model-based RL (or identify that an RL agent has a model).
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Model-based+Policy+Optimization+with+Unsupervised+Model+Adaptation&btnG="><span>Jian Shen, Han Zhao, Weinan Zhang, Yong Yu<br/><i>Model-based Policy Optimization with Unsupervised Model Adaptation</i>, 2020</span>Jian Shen et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_1dc3a89d0d440ba31729b0ba74b93a33.html">[poster]</a></sup>, learn to minimize distributional mismatch been real and generated data in the encoding space, makes better model-based planning agents.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Efficient+model-based+reinforcement+learning+through+optimistic+policy+search+and+planning&btnG="><span>Sebastian Curi, Felix Berkenkamp, Andreas Krause<br/><i>Efficient model-based reinforcement learning through optimistic policy search and planning</i>, 2020</span>Sebastian Curi et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_a36b598abb934e4528412e5a2127b931.html">[poster]</a></sup>, if we learn epistemic and aleatoric uncertainty, instead of planning wrt both uncertainties in expectation, greedy plan and ``hallucinate control directly on the epistemic uncertainty''. I think?
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Forethought+and+Hindsight+in+Credit+Assignment&btnG="><span>Veronica Chelu, Doina Precup, Hado P van Hasselt<br/><i>Forethought and Hindsight in Credit Assignment</i>, 2020</span>Veronica Chelu et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_18064d61b6f93dab8681a460779b8429.html">[poster]</a></sup>, forward vs backward models,``if the future is broad and predictable, forethought is invaluable, if backward hypotheses are causal and less determined by chance, hindsight planning is effective, states we pick to plan from matter''.
  </li><li>-- Day 2 --
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Model-based+Adversarial+Meta-Reinforcement+Learning&btnG="><span>Zichuan Lin, Garrett Thomas, Guangwen Yang, Tengyu Ma<br/><i>Model-based Adversarial Meta-Reinforcement Learning</i>, 2020</span>Zichuan Lin et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_73634c1dcbe056c1f7dcf5969da406c8.html">[poster]</a></sup>, meta-RL, learn a model conditioned on the task, find an adversarial task and condition the learning subprocess to be robust to it through planning.
  </li><li>-- Day 3 --
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=PlanGAN:+Model-based+Planning+With+Sparse+Rewards+and+Multiple+Goals&btnG="><span>Henry Charlesworth, Giovanni Montana<br/><i>PlanGAN: Model-based Planning With Sparse Rewards and Multiple Goals</i>, 2020</span>Henry Charlesworth et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_6101903146e4bbf4999c449d78441606.html">[poster]</a></sup>, learn goal-conditioned GAN to generate multiple trajectories (<span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>s</mi></mrow><annotation encoding="application/x-tex">s</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">s</span></span></span></span> and <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>a</mi></mrow><annotation encoding="application/x-tex">a</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.43056em;vertical-align:0em;"></span><span class="mord mathnormal">a</span></span></span></span>) and use these trajectories to train in hindsight. (robotics, 4rooms)
    </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Latent+World+Models+For+Intrinsically+Motivated+Exploration&btnG="><span>Aleksandr Ermolov, Nicu Sebe<br/><i>Latent World Models For Intrinsically Motivated Exploration</i>, 2020</span>Aleksandr Ermolov et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_3c09bb10e2189124fdd8f467cc8b55a7.html">[poster]</a></sup>, learn time-contractive representations, learn forward model, add model error as an intrinsic reward, get good exploration (Montezuma's revenge)
</li></ul><br/>

<a name="s30"></a><h4>9.4 Goal-based/hindsight</h4><br/>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Generalized+Hindsight+for+Reinforcement+Learning&btnG="><span>Alexander C Li, Lerrel Pinto, Pieter Abbeel<br/><i>Generalized Hindsight for Reinforcement Learning</i>, 2020</span>Alexander C Li et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_57e5cb96e22546001f1d6520ff11d9ba.html">[poster]</a></sup>, generalize HER to tasks (non-sparse reward functions rather than goal space=state space). Task relabeling is also changed, relabel trajectories by picking reward fcts which are likely to have generated them (drawing from inverse RL).
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Rewriting+History+with+Inverse+RL:+Hindsight+Inference+for+Policy+Improvement&btnG="><span>Benjamin Eysenbach, Xinyang Geng, Sergey Levine, Ruslan Salakhutdinov<br/><i>Rewriting History with Inverse RL: Hindsight Inference for Policy Improvement</i>, 2020</span>Benjamin Eysenbach et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_a97da629b098b75c294dffdc3e463904.html">[poster]</a></sup>, relabel trajectories with inverse RL. So basically like the previous paper... coincidence? Berkeley is too big haha.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Value-driven+Hindsight+Modelling&btnG="><span>Arthur Guez, Fabio Viola, Théophane Weber, Lars Buesing, Steven Kapturowski, Doina Precup, David Silver, Nicolas Heess<br/><i>Value-driven Hindsight Modelling</i>, 2020</span>Arthur Guez et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_9381fc93ad66f9ec4b2eef71147a6665.html">[poster]</a></sup>, learn to predict <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>V</mi></mrow><annotation encoding="application/x-tex">V</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.68333em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.22222em;">V</span></span></span></span> from the future <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>τ</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">\tau^+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.771331em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span></span></span></span> yielding an embedding <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ϕ</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">\phi^+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9657709999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span></span></span></span>, then learn to predict <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>ϕ</mi><mo>+</mo></msup></mrow><annotation encoding="application/x-tex">\phi^+</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.9657709999999999em;vertical-align:-0.19444em;"></span><span class="mord"><span class="mord mathnormal">ϕ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">+</span></span></span></span></span></span></span></span></span></span></span> from the past <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>τ</mi><mo>−</mo></msup></mrow><annotation encoding="application/x-tex">\tau^-</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.771331em;vertical-align:0em;"></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.1132em;">τ</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.771331em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mbin mtight">−</span></span></span></span></span></span></span></span></span></span></span> yielding <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>ϕ</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1523199999999998em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">ϕ</span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span></span>, then use <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover accent="true"><mi>ϕ</mi><mo>^</mo></mover></mrow><annotation encoding="application/x-tex">\hat\phi</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.1523199999999998em;vertical-align:-0.19444em;"></span><span class="mord accent"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.9578799999999998em;"><span style="top:-3em;"><span class="pstrut" style="height:3em;"></span><span class="mord mathnormal">ϕ</span></span><span style="top:-3.26344em;"><span class="pstrut" style="height:3em;"></span><span class="accent-body" style="left:-0.16666em;"><span class="mord">^</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.19444em;"><span></span></span></span></span></span></span></span></span> to compute the value function as usual. ``Reasoning, in hindsight, about which aspects of the future observations could help past value prediction''
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Automatic+curriculum+learning+through+value+disagreement&btnG="><span>Yunzhi Zhang, Pieter Abbeel, Lerrel Pinto<br/><i>Automatic curriculum learning through value disagreement</i>, 2020</span>Yunzhi Zhang et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_566f0ea4f6c2e947f36795c8f58ba901.html">[poster]</a></sup>, ``we introduce a goal proposal module that prioritizes goals that maximize the epistemic uncertainty of the Q-function of the policy''.
</li></ul><br/>

<a name="s31"></a><h4>9.5 Deep Learning</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+Numerosity+of+Deep+Neural+Networks&btnG="><span>Xi Zhang, Xiaolin Wu<br/><i>On Numerosity of Deep Neural Networks</i>, 2020</span>Xi Zhang et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_13e36f06c66134ad65f532e90d898545.html">[poster]</a></sup>, DNNs don't naturally learn to count, unlike previously claimed (even if they are trained to count, mixed results).
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Exactly+Computing+the+Local+Lipschitz+Constant+of+ReLU+Networks&btnG="><span>Matt Jordan, Alexandros G Dimakis<br/><i>Exactly Computing the Local Lipschitz Constant of ReLU Networks</i>, 2020</span>Matt Jordan et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_5227fa9a19dce7ba113f50a405dcaf09.html">[poster]</a></sup>, compute (exactly??) the Lipschitz constant of ReLU networks, shows it increases during training, and shows that different regularization schemes affect the growth rate.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=The+generalization-stability+tradeoff+in+neural+network+pruning&btnG="><span>Brian Bartoldson, Ari Morcos, Adrian Barbu, Gordon Erlebacher<br/><i>The generalization-stability tradeoff in neural network pruning</i>, 2020</span>Brian Bartoldson et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_ef2ee09ea9551de88bc11fd7eeea93b0.html">[poster]</a></sup>, pruning can actually improve test acc. When pruning pre-finetuning reduces test accuracy more, that's usually when it increases it the most post-finetuning: ``less pruning stability leads to more model flatness''. Link between pruning and the generalization of overparameterized DNNs.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Hierarchical+nucleation+in+deep+neural+networks&btnG="><span>Diego Doimo, Aldo Glielmo, Alessio Ansuini, Alessandro Laio<br/><i>Hierarchical nucleation in deep neural networks</i>, 2020</span>Diego Doimo et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_54f3bc04830d762a3b56a789b6ff62df.html">[poster]</a></sup>, cool viz paper showing that on Imagenet categories are learn hierarchically in time and embedding space. Also some comment on depth (lower layers learn common features, higher layers learn separable features).
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Optimal+Lottery+Tickets+via+Subset+Sum:+Logarithmic+Over-Parameterization+is+Sufficient&btnG="><span>Ankit Pensia, Shashank Rajput, Alliot Nagle, Harit Vishwakarma, Dimitris Papailiopoulos<br/><i>Optimal Lottery Tickets via Subset Sum: Logarithmic Over-Parameterization is Sufficient</i>, 2020</span>Ankit Pensia et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_1b742ae215adf18b75449c6e272fd92d.html">[poster]</a></sup>, lottery ticket, ``any target network of width <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>d</mi></mrow><annotation encoding="application/x-tex">d</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal">d</span></span></span></span> and depth <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>l</mi></mrow><annotation encoding="application/x-tex">l</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.69444em;vertical-align:0em;"></span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span></span></span></span> can be approximated by pruning a random network that is a factor <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><mi>l</mi><mi>o</mi><mi>g</mi><mo stretchy="false">(</mo><mi>d</mi><mi>l</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(log(dl))</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mord mathnormal">o</span><span class="mord mathnormal" style="margin-right:0.03588em;">g</span><span class="mopen">(</span><span class="mord mathnormal">d</span><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span> wider and twice as deep''. Solid improvement over <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo stretchy="false">(</mo><msup><mi>d</mi><mn>4</mn></msup><msup><mi>l</mi><mn>2</mn></msup><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">O(d^4l^2)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.064108em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.02778em;">O</span><span class="mopen">(</span><span class="mord"><span class="mord mathnormal">d</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">4</span></span></span></span></span></span></span></span><span class="mord"><span class="mord mathnormal" style="margin-right:0.01968em;">l</span><span class="msupsub"><span class="vlist-t"><span class="vlist-r"><span class="vlist" style="height:0.8141079999999999em;"><span style="top:-3.063em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mtight">2</span></span></span></span></span></span></span></span><span class="mclose">)</span></span></span></span>!
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+the+distance+between+two+neural+networks+and+the+stability+of+learning&btnG="><span>Jeremy Bernstein, Arash Vahdat, Yisong Yue, Ming-Yu Liu<br/><i>On the distance between two neural networks and the stability of learning</i>, 2020</span>Jeremy Bernstein et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_f4b31bee138ff5f7b84ce1575a738f95.html">[poster]</a></sup>, a distance between neural networks (same arch) based on normalized Froebius norm product. Weird? Induces a learning method that is more stable than SGD/Adam.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Greedy+Optimization+Provably+Wins+the+Lottery:+Logarithmic+Number+of+Winning+Tickets+is+Enough&btnG="><span>Mao Ye, Lemeng Wu, Qiang Liu<br/><i>Greedy Optimization Provably Wins the Lottery: Logarithmic Number of Winning Tickets is Enough</i>, 2020</span>Mao Ye et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_be23c41621390a448779ee72409e5f49.html">[poster]</a></sup>, log-time algorithm to find lottery ticket.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=What+Do+Neural+Networks+Learn+When+Trained+With+Random+Labels?&btnG="><span>Hartmut Maennel, Ibrahim M Alabdulmohsin, Ilya O Tolstikhin, Robert Baldock, Olivier Bousquet, Sylvain Gelly, Daniel Keysers<br/><i>What Do Neural Networks Learn When Trained With Random Labels?</i>, 2020</span>Hartmut Maennel et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_e4191d610537305de1d294adb121b513.html">[poster]</a></sup>, train DNN on random labels, then retrain on different data, it's faster! So DNN must have learned something. In fact learns principal components. Effect exists mainly for lower layers.
</li><li>-- Day 2 --
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=What+is+being+transferred+in+transfer+learning?&btnG="><span>Behnam Neyshabur, Hanie Sedghi, Chiyuan Zhang<br/><i>What is being transferred in transfer learning?</i>, 2020</span>Behnam Neyshabur et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_0607f4c705595b911a4f3e7a127b44e0.html">[poster]</a></sup> transfer learning, ``when training from pre-trained weights, the model stays in the same basin in the loss landscape and different instances of such model are similar in feature space and close in parameter space''
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=A+closer+look+at+accuracy+vs.+robustness&btnG="><span>Yao-Yuan Yang, Cyrus Rashtchian, Hongyang Zhang, Russ R Salakhutdinov, Kamalika Chaudhuri<br/><i>A closer look at accuracy vs. robustness</i>, 2020</span>Yao-Yuan Yang et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_61d77652c97ef636343742fc3dcf3ba9.html">[poster]</a></sup>  ``achieving robustness and accuracy in practice may require using methods that impose local Lipschitzness and augmenting them with deep learning generalization techniques'', not sure what they mean by generalization techniques.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Adversarial+Weight+Perturbation+Helps+Robust+Generalization&btnG="><span>Dongxian Wu, Shu-Tao Xia, Yisen Wang<br/><i>Adversarial Weight Perturbation Helps Robust Generalization</i>, 2020</span>Dongxian Wu et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_1ef91c212e30e14bf125e9374262401f.html">[poster]</a></sup>, train on adversarial weight perturbations (+ the usual adversarial inputs), improves generalisation and robustness.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=When+do+neural+networks+outperform+kernel+methods?&btnG="><span>Behrooz Ghorbani, Song Mei, Theodor Misiakiewicz, Andrea Montanari<br/><i>When do neural networks outperform kernel methods?</i>, 2020</span>Behrooz Ghorbani et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_a9df2255ad642b923d95503b9a7958d8.html">[poster]</a></sup>, when are DNNs not kernels? When the signal-to-noise ratio is high, DNNs can can capture high-frequency patterns which kernel methods cannot (e.g. complicated natural images?)
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Understanding+Double+Descent+Requires+a+Fine-Grained+Bias-Variance+Decomposition&btnG="><span>Ben Adlam, Jeffrey Pennington<br/><i>Understanding Double Descent Requires a Fine-Grained Bias-Variance Decomposition</i>, 2020</span>Ben Adlam et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_7d420e2b2939762031eed0447a9be19f.html">[poster]</a></sup>, double descent occurs because of two (interaction) terms within the bias-variance decomposition of the loss. It makes sense that adding label noise amplifies double-descent but it has other causes.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=MomentumRNN:+Integrating+Momentum+into+Recurrent+Neural+Networks&btnG="><span>Tan Nguyen, Richard Baraniuk, Andrea Bertozzi, Stanley Osher, Bao Wang<br/><i>MomentumRNN: Integrating Momentum into Recurrent Neural Networks</i>, 2020</span>Tan Nguyen et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_149ef6419512be56a93169cd5e6fa8fd.html">[poster]</a></sup>, momentum-RNN, adds cell that's like momentum into a GRU or LSTM-like recurrence. Forces some stability of information propagation and gradient.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=The+pitfalls+of+simplicity+bias+in+neural+networks&btnG="><span>Harshay Shah, Kaustav Tamuly, Aditi Raghunathan, Prateek Jain, Praneeth Netrapalli<br/><i>The pitfalls of simplicity bias in neural networks</i>, 2020</span>Harshay Shah et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_6cfe0e6127fa25df2a0ef2ae1067d915.html">[poster]</a></sup>, ``the Simplicity Bias of SGD and variants can be extreme: neural networks can exclusively rely on the simplest feature and remain invariant to all predictive complex features.''
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Gradient+estimation+with+stochastic+softmax+tricks&btnG="><span>Max Paulus, Dami Choi, Daniel Tarlow, Andreas Krause, Chris J Maddison<br/><i>Gradient estimation with stochastic softmax tricks</i>, 2020</span>Max Paulus et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_3df80af53dce8435cf9ad6c3e7a403fd.html">[poster]</a></sup>, generalization of Gumbel-softmax trick to more structures. Still needs a softened equivalent structure, so no RL.
</li><li>-- Day 3 --
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=How+Can+I+Explain+This+to+You?+An+Empirical+Study+of+Deep+Neural+Network+Explanation+Methods&btnG="><span>Jeya Vikranth Jeyakumar, Joseph Noor, Yu-Hsi Cheng, Luis Garcia, Mani Srivastava<br/><i>How Can I Explain This to You? An Empirical Study of Deep Neural Network Explanation Methods</i>, 2020</span>Jeya Vikranth Jeyakumar et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_2c29d89cc56cdb191c60db2f0bae796b.html">[poster]</a></sup>, a review of DNN explanation methods, for visual inputs, explanation by example seems the best (mechanical turk).
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Neural+Complexity+Measures&btnG="><span>Yoonho Lee, Juho Lee, Sung Ju Hwang, Eunho Yang, Seungjin Choi<br/><i>Neural Complexity Measures</i>, 2020</span>Yoonho Lee et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_6e17a5fd135fcaf4b49f2860c2474c7c.html">[poster]</a></sup>, can meta-learn the generalization gap, make it a function of model and data, and even use that function to regularize training for new models.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+the+training+dynamics+of+deep+networks+with+$+L\_2+$+regularization&btnG="><span>Aitor Lewkowycz, Guy Gur-Ari<br/><i>On the training dynamics of deep networks with $ L\_2 $ regularization</i>, 2020</span>Aitor Lewkowycz et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_32fcc8cfe1fa4c77b5c58dafd36d1a98.html">[poster]</a></sup>, relationship between L2 and optimal early stop, large L2 is worse but converges faster, so start with big L2 then decay.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Bootstrap+your+own+latent-a+new+approach+to+self-supervised+learning&btnG="><span>Jean-Bastien Grill, Florian Strub, Florent Altché, Corentin Tallec, Pierre Richemond, Elena Buchatskaya, Carl Doersch, Bernardo Avila Pires, Zhaohan Guo, Mohammad Gheshlaghi Azar, others<br/><i>Bootstrap your own latent-a new approach to self-supervised learning</i>, 2020</span>Jean-Bastien Grill et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_f3ada80d5c4ee70142b17b8192b2958e.html">[poster]</a></sup>, self-supervised learning without negative pairs by keeping an exponential moving averaged target network (like in DDQN!) and predicting its latent variables when show the same image but transformed.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Compositional+explanations+of+neurons&btnG="><span>Jesse Mu, Jacob Andreas<br/><i>Compositional explanations of neurons</i>, 2020</span>Jesse Mu et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_c74956ffb38ba48ed6ce977af6727275.html">[poster]</a></sup>, interpret the neurons of a DNN as interpreting some kind of logical statement (A and (B or C)) about the input. This yields an interesting compositional interpretation mechanism.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=On+Warm-Starting+Neural+Network+Training&btnG="><span>Jordan Ash, Ryan P Adams<br/><i>On Warm-Starting Neural Network Training</i>, 2020</span>Jordan Ash et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_288cd2567953f06e460a33951f55daaf.html">[poster]</a></sup>, when getting new data, instead of retraining from scratch, can train from previous converged parameters, but this generalizes poorly. Proposal: shrink and perturb when getting new data. (could this be used for DQN-style methods?)
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Towards+Theoretically+Understanding+Why+SGD+Generalizes+Better+Than+ADAM+in+Deep+Learning&btnG="><span>Pan Zhou, Jiashi Feng, Chao Ma, Caiming Xiong, Steven Chu Hong Hoi, others<br/><i>Towards Theoretically Understanding Why SGD Generalizes Better Than ADAM in Deep Learning</i>, 2020</span>Pan Zhou et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_f3f27a324736617f20abbf2ffd806f6d.html">[poster]</a></sup>, why does Adam generalize worse than SGD? Adam flattens basins, destroys the anisotropic nature of gradients, making the escape time from said basins longer. This may be especially true of sharp basins/minima, associated with overfitting.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Estimating+Training+Data+Influence+by+Tracing+Gradient+Descent&btnG="><span>Garima Pruthi, Frederick Liu, Satyen Kale, Mukund Sundararajan<br/><i>Estimating Training Data Influence by Tracing Gradient Descent</i>, 2020</span>Garima Pruthi et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_e6385d39ec9394f2f3a354d9d2b88eec.html">[poster]</a></sup> another method of per-example influence on model (uses interference I think?)
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Deep+learning+versus+kernel+learning:+an+empirical+study+of+loss+landscape+geometry+and+the+time+evolution+of+the+Neural+Tangent+Kernel&btnG="><span>Stanislav Fort, Gintare Karolina Dziugaite, Mansheej Paul, Sepideh Kharaghani, Daniel M Roy, Surya Ganguli<br/><i>Deep learning versus kernel learning: an empirical study of loss landscape geometry and the time evolution of the Neural Tangent Kernel</i>, 2020</span>Stanislav Fort et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_405075699f065e43581f27d67bb68478.html">[poster]</a></sup>, if you linearize a model for training early, it doesn't learn very well, but if you wait a bit more, the later models linearize very well (NTK). They end up in the same/similar basins.
</li></ul><br/>
<a name="s32"></a><h4>9.6 Learning Theory</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Model+Interpretability+through+the+lens+of+Computational+Complexity&btnG="><span>Pablo Barceló, Mikaël Monet, Jorge Pérez, Bernardo Subercaseaux<br/><i>Model Interpretability through the lens of Computational Complexity</i>, 2020</span>Pablo Barceló et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_b1adda14824f50ef24ff1c05bb66faf3.html">[poster]</a></sup>, propose to consider the computational complexity of interpreting a given model as its ``interpretability''. Find that linear <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>≈</mo></mrow><annotation encoding="application/x-tex">\approx</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.48312em;vertical-align:0em;"></span><span class="mrel">≈</span></span></span></span> tree-based, that shallow <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>&lt;</mo></mrow><annotation encoding="application/x-tex">&lt;</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:0.5782em;vertical-align:-0.0391em;"></span><span class="mrel">&lt;</span></span></span></span> deep neural nets, which is somewhat consistent with prior notions of interpretability.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=In+search+of+robust+measures+of+generalization&btnG="><span>Gintare Karolina Dziugaite, Alexandre Drouin, Brady Neal, Nitarshan Rajkumar, Ethan Caballero, Linbo Wang, Ioannis Mitliagkas, Daniel M Roy<br/><i>In search of robust measures of generalization</i>, 2020</span>Gintare Karolina Dziugaite et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_86d7c8a08b4aaa1bc7c599473f5dddda.html">[poster]</a></sup>, generalization theories should be judged by worst-case performance over a set of environments. Propose distributional framework that is robust to hyperparameter <i>change</i>. No known measure is robust. Average case performance can be misleading.
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Implicit+Regularization+in+Deep+Learning+May+Not+Be+Explainable+by+Norms&btnG="><span>Noam Razin, Nadav Cohen<br/><i>Implicit Regularization in Deep Learning May Not Be Explainable by Norms</i>, 2020</span>Noam Razin et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_f21e255f89e0f258accbe4e984eef486.html">[poster]</a></sup> ``rather than perceiving the implicit regularization of SGD via norms, a potentially more useful interpretation is minimization of rank''
</li></ul><br/>
<a name="s33"></a><h4>9.7 AI Alignment</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Consequences+of+Misaligned+AI&btnG="><span>Simon Zhuang, Dylan Hadfield-Menell<br/><i>Consequences of Misaligned AI</i>, 2020</span>Simon Zhuang et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_b607ba543ad05417b8507ee86c54fcb7.html">[poster]</a></sup>, if the agent doesn't have the full information about the reward/utility, it can learn solutions of arbitrary low utility. ``we should view the design of reward functions as an interactive and dynamic process and identifies a theoretical scenario where some degree of interactivity is desirable.''
</li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Avoiding+Side+Effects+in+Complex+Environments&btnG="><span>Alexander Matt Turner, Neale Ratzlaff, Prasad Tadepalli<br/><i>Avoiding Side Effects in Complex Environments</i>, 2020</span>Alexander Matt Turner et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_f50a6c02a3fc5a3a5d4d9391f05f3efc.html">[poster]</a></sup>, ``Attainable Utility Preservation (AUP) avoided side effects by penalizing shifts in the ability to achieve randomly generated goals'', cool env as well.
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=AvE:+Assistance+via+Empowerment&btnG="><span>Yuqing Du, Stas Tiomkin, Emre Kiciman, Daniel Polani, Pieter Abbeel, Anca Dragan<br/><i>AvE: Assistance via Empowerment</i>, 2020</span>Yuqing Du et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_30de9ece7cf3790c8c39ccff1a044209.html">[poster]</a></sup>, human-machine interaction can be aided by maximizing the human user's empowerment. This acts as a reward which is easier/less ambiguous than a possibly ill-defined goal.
</li></ul><br/>
<a name="s34"></a><h4>9.8 Datasets</h4>
<ul><li><a class="tooltip" href="https://scholar.google.com/scholar?q=Synbols:+Probing+learning+algorithms+with+synthetic+datasets&btnG="><span>Alexandre Lacoste, Pau Rodríguez López, Frédéric Branchaud-Charron, Parmida Atighehchian, Massimo Caccia, Issam Hadj Laradji, Alexandre Drouin, Matthew Craddock, Laurent Charlin, David Vázquez<br/><i>Synbols: Probing learning algorithms with synthetic datasets</i>, 2020</span>Alexandre Lacoste et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_0169cf885f882efd795951253db5cdfb.html">[poster]</a></sup>, synbols, synthetic dataset to understand what DNNs are robust to.
</li><li>RL Unplugged, DM set of offline data to benchmark RL
  </li><li><a class="tooltip" href="https://scholar.google.com/scholar?q=The+nethack+learning+environment&btnG="><span>Heinrich Küttler, Nantas Nardelli, Alexander Miller, Roberta Raileanu, Marco Selvatici, Edward Grefenstette, Tim Rocktäschel<br/><i>The nethack learning environment</i>, 2020</span>Heinrich Küttler et al. (2020)</a><sup><a href="https://neurips.cc/virtual/2020/protected/poster_569ff987c643b4bedf504efda8f786c2.html">[poster]</a></sup>, NetHack ¯\_(ツ)_/¯</li></ul><br/>
<a name="s35"></a><h4>9.9 Workshops</h4><br/>
<ul><li><a href=https://slideslive.com/38941301/domain-adversarial-reinforcement-learning>https://slideslive.com/38941301/domain-adversarial-reinforcement-learning</a></li><li><a href=https://slideslive.com/38941275/learning-to-reach-goals-via-iterated-supervised-learning>https://slideslive.com/38941275/learning-to-reach-goals-via-iterated-supervised-learning</a></li><li><a href=https://slideslive.com/38941375/distributionconditioned-reinforcement-learning>https://slideslive.com/38941375/distributionconditioned-reinforcement-learning</a></li><li><a href=https://slideslive.com/38938066/bioconstrained-intelligence-and-the-great-filter>https://slideslive.com/38938066/bioconstrained-intelligence-and-the-great-filter</a> :)
</li></ul><br/>

<br/>

<br/>

<br/>

<br/>

<br/>

<br/>

<br/>

<br/>

<br/>

<br/>


   </div>
   <div style='height: 10em;'></div>
  </body>
</html>
