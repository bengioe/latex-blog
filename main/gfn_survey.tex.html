
<!DOCTYPE html>
<meta charset="utf-8">
<html>
  <head>
    <title>An informal survey of Generative Flow Network literature</title>
    <link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">
    <script src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>
    <script src="https://fpcdn.s3.amazonaws.com/apps/polygon-tools/0.4.6/polygon-tools.min.js" type="text/javascript"></script>
    
    <link rel="stylesheet" href="main.css">
    <meta name="viewport" content="width=device-width, initial-scale=1">
    
  </head>
  <body>
   <div class="content">
   <center><a href="http://folinoid.com/">[Home]</a></center><br/><br/>
     





<center><a name="s1"></a><h3> An informal survey of Generative Flow Network literature</h3></center>

<center>Emmanuel Bengio</center>
<br/><br/><br/>
What follows is my attempt to force myself to keep track of GFlowNet (GFN) literature. (Un)fortunately, this may very well get out of hand as GFNs appear to gain in popularity. I hope this summary is helpful, and as always, check links and citations within the cited papers for more context. Happy hacking.<br/><br/>

<ul class="toc"><li> <a href="#s1">An informal survey of Generative Flow Network literature</a></li>
<li>1 <a href="#s2">The roots</a></li>
<li>1.1 <a href="#s3">So, what's a GFlowNet?</a></li>
<li>1.2 <a href="#s4">Training GFlowNets</a></li></ul>

<a name="s2"></a><h3>1 The roots</h3>

Generative Flow Networks (GFNs) were introduced by <a class="tooltip" href="https://arxiv.org/abs/2106.04399"><span>Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, Yoshua Bengio<br/><i>Flow network based generative models for non-iterative diverse candidate generation</i>, 2021</span>Emmanuel Bengio et al. (2021)</a>. Their creation came from the desire to make reinforcement learning methods a bit more exploratory, by design, in a context of discrete optimization. Thanks a few extra assumptions on the Markov Decision Process and a different learning objective, a trained GFN model samples (i.i.d.) from some unnormalized distribution over objects.<br/><br/>
<a name="s3"></a><h4>1.1 So, what's a GFlowNet?</h4>

There are a number of introductory materials on GFNs, but I'll point out the thorough <a href=http://tinyurl.com/gflownet-tutorial>tutorial written by Yoshua, Kolya Malkin, and Moksh Jain</a>, as well as this <a href=https://colab.research.google.com/drive/1fUMwgu2OhYpQagpzU5mhe9_Esib3Q2VR#scrollTo=gA3nVc6hnjY3>Colab tutorial</a> I wrote to introduce GFNs through code.<br/><br/>
The intuitive idea behind a GFN is to estimate flows (of water, of particles) in a pointed directed acyclic <i>network</i>. Perhaps confusingly, the <i>Network</i> in GF<i>N</i> refers to the state space, <i>not</i> a neural network architecture. The network represents all possible ways of constructing an object, and so knowing the flow gives us a policy which we can follow to sequentially construct objects. What, we believe, makes the strength of GFNs is that at convergence, this policy yields samples in proportion to an energy function, i.e. <span class="katex-display"><span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML" display="block"><semantics><mtable rowspacing="0.24999999999999992em" columnalign="right" columnspacing=""><mtr><mtd><mstyle scriptlevel="0" displaystyle="true"><mrow><msub><mi>p</mi><mi>θ</mi></msub><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>∝</mo><mi>e</mi><mi>x</mi><mi>p</mi><mo stretchy="false">(</mo><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo stretchy="false">)</mo></mrow></mstyle></mtd></mtr></mtable><annotation encoding="application/x-tex">\begin{aligned}p_\theta(x) \propto exp(f(x))\end{aligned}</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1.5000000000000002em;vertical-align:-0.5000000000000002em;"></span><span class="mord"><span class="mtable"><span class="col-align-r"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:1em;"><span style="top:-3.16em;"><span class="pstrut" style="height:3em;"></span><span class="mord"><span class="mord"><span class="mord mathnormal">p</span><span class="msupsub"><span class="vlist-t vlist-t2"><span class="vlist-r"><span class="vlist" style="height:0.33610799999999996em;"><span style="top:-2.5500000000000003em;margin-left:0em;margin-right:0.05em;"><span class="pstrut" style="height:2.7em;"></span><span class="sizing reset-size6 size3 mtight"><span class="mord mathnormal mtight" style="margin-right:0.02778em;">θ</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.15em;"><span></span></span></span></span></span></span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∝</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mord mathnormal">e</span><span class="mord mathnormal">x</span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mclose">)</span></span></span></span><span class="vlist-s">​</span></span><span class="vlist-r"><span class="vlist" style="height:0.5000000000000002em;"><span></span></span></span></span></span></span></span></span></span></span></span>

Due to some accidents of history, and my own RL ancestry, we often refer in GFN papers to a reward function <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>&gt;</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">R(x)&gt;0</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">&gt;</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:0.64444em;vertical-align:0em;"></span><span class="mord">0</span></span></span></span>, and therefore to <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>∝</mo><mi>R</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">p(x)\propto R(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal">p</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">∝</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>, making <span class="katex"><span class="katex-mathml"><math xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>f</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo><mo>=</mo><mi>log</mi><mo>⁡</mo><mi>R</mi><mo stretchy="false">(</mo><mi>x</mi><mo stretchy="false">)</mo></mrow><annotation encoding="application/x-tex">f(x) = \log R(x)</annotation></semantics></math></span><span class="katex-html" aria-hidden="true"><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mord mathnormal" style="margin-right:0.10764em;">f</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span><span class="mrel">=</span><span class="mspace" style="margin-right:0.2777777777777778em;"></span></span><span class="base"><span class="strut" style="height:1em;vertical-align:-0.25em;"></span><span class="mop">lo<span style="margin-right:0.01389em;">g</span></span><span class="mspace" style="margin-right:0.16666666666666666em;"></span><span class="mord mathnormal" style="margin-right:0.00773em;">R</span><span class="mopen">(</span><span class="mord mathnormal">x</span><span class="mclose">)</span></span></span></span>. In RL-speak, we're able to use the GFN framework to find a policy that maximizes the entropy of the terminal state distribution, in a terminal-reward episodic DAG-MDP, with an off-policy offline objective.<br/><br/>
While the content of our original paper (<a class="tooltip" href="https://arxiv.org/abs/2106.04399"><span>Emmanuel Bengio, Moksh Jain, Maksym Korablyov, Doina Precup, Yoshua Bengio<br/><i>Flow network based generative models for non-iterative diverse candidate generation</i>, 2021</span>Emmanuel Bengio et al., 2021</a>) was mostly correct, the work of <a class="tooltip" href="https://arxiv.org/abs/2111.09266"><span>Yoshua Bengio, Tristan Deleu, Edward J Hu, Salem Lahlou, Mo Tiwari, Emmanuel Bengio<br/><i>Gflownet foundations</i>, 2021</span>Yoshua Bengio et al. (2021)</a> solidifies the maths of the GFN framework into a coherent theory.<br/><br/>
<a class="tooltip" href="https://arxiv.org/abs/2111.09266"><span>Yoshua Bengio, Tristan Deleu, Edward J Hu, Salem Lahlou, Mo Tiwari, Emmanuel Bengio<br/><i>Gflownet foundations</i>, 2021</span>Yoshua Bengio et al. (2021)</a> reintroduce GFlowNets through the concept of trajectory flows in pointed DAGs, Markovian flows, the probability measures that they induce, and the properties that they have. There are lots more interesting properties derived there than I will list here. Among multiple other things, this work speculates on a series of ideas that GFNs are compatible with; GFNs can be conditioned easily, on events or intial states which can yield quantities such as the free energy or entropy of a state, or on a reward function or its description; GFNs can be used to generate sets, graphs, or to marginalize joint distributions; GFNs can also be used to induce a distribution over a (smoothed) Pareto front.<br/><br/>
<a name="s4"></a><h4>1.2 Training GFlowNets</h4>


   </div>
   <div style='height: 10em;'></div>
  </body>
</html>
